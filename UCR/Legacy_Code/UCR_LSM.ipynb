{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSVData(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for TSV files where:\n",
    "      - The first column is the label (-1 for abnormal, 1 for normal)\n",
    "      - The remaining columns are numeric features (each sample is a time series).\n",
    "      \n",
    "    The dataset:\n",
    "      1. Reads the TSV file using pandas.\n",
    "      2. Converts labels from {-1, 1} to {0, 1} (with 0=abnormal, 1=normal).\n",
    "      3. Reshapes the features so that each sample is of shape [time_steps, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "        raw_labels = self.data.iloc[:, 0].values.astype(int)\n",
    "        # Convert: -1 -> 0 (abnormal) and 1 -> 1 (normal)\n",
    "        self.labels = ((raw_labels == 1).astype(int))\n",
    "        self.features = self.data.iloc[:, 1:].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Each sample: shape [time_steps] --> reshape to [time_steps, 1]\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32).unsqueeze(1)\n",
    "        # For CrossEntropyLoss, labels should be long integers\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liquid State Machine - Spiking Reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingReservoir(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,         \n",
    "                 reservoir_size,    \n",
    "                 num_readout,      \n",
    "                 num_connected,     \n",
    "                 spectral_radius,    \n",
    "                 beta_input,         \n",
    "                 beta_reservoir,     \n",
    "                 threshold,          \n",
    "                 sparsity_percentage,# percent of recurrent connections to drop, for sparsity (e.g., 10 for 10%)\n",
    "                 spike_grad=None,    # gradient surrogate (None = arctan, or I can specify something like surrogate gradient)\n",
    "                 reset_mechanism='zero',  # or 'subtract'\n",
    "                 reset_delay=0):     # delay before resetting membrane potential, so it can spike when reaching threshold or the next time step\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(1, 1, bias=False)\n",
    "        self.input_lif = snn.Leaky(beta=beta_input,\n",
    "                                   threshold=threshold,\n",
    "                                   spike_grad=spike_grad,\n",
    "                                   reset_mechanism=reset_mechanism,\n",
    "                                   reset_delay=reset_delay)\n",
    "        \n",
    "        self.reservoir_fc = nn.Linear(1, reservoir_size, bias=False)\n",
    "        self.reservoir_lif = snn.RLeaky(beta=beta_reservoir,\n",
    "                                        linear_features=reservoir_size,\n",
    "                                        threshold=threshold,\n",
    "                                        spike_grad=spike_grad,\n",
    "                                        reset_mechanism=reset_mechanism,\n",
    "                                        reset_delay=reset_delay,\n",
    "                                        all_to_all=True)\n",
    "        # sparsity mask: keep (100 - sparsity_percentage)% of connections to create sparsity\n",
    "        sparsity_mask = (torch.rand(reservoir_size, reservoir_size) >\n",
    "                         (sparsity_percentage/100)).float()\n",
    "        self.register_buffer(\"sparsity_mask\", sparsity_mask)\n",
    "        with torch.no_grad():\n",
    "            self.reservoir_lif.recurrent.weight.mul_(self.sparsity_mask) # mul_ is in-place multiplication, so it modifies the tensor in place without creating a new tensor\n",
    "            W = self.reservoir_lif.recurrent.weight\n",
    "            # torch.linalg.eigvals returns complex eigenvalues; taking the maximum absolute value for the spectral radius\n",
    "            current_radius = torch.linalg.eigvals(W).abs().max()\n",
    "            scaling_factor = spectral_radius / current_radius\n",
    "            W.mul_(scaling_factor)\n",
    "        \n",
    "        # for readout layer I select a subset of reservoir neurons (randomly, dependent on seed, so seed can be a hyperparam)\n",
    "        readout_indices = torch.randperm(reservoir_size)[:num_connected]\n",
    "        self.register_buffer(\"readout_indices\", readout_indices) # register_buffer is used to store tensors that are not trainable\n",
    "        \n",
    "        self.readout_fc = nn.Linear(num_connected, num_readout, bias=True)\n",
    "        \n",
    "        # freeze the parameters of all layers except the readout so they are not trained\n",
    "        for param in self.input_fc.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.input_lif.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.reservoir_fc.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.reservoir_lif.parameters():\n",
    "            param.requires_grad = False\n",
    "        # only self.readout_fc remains trainable\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: tensor of shape [batch_size, time_steps, 1]\n",
    "        Processes the time series sequentially.\n",
    "        \n",
    "        Returns:\n",
    "          logits: [batch_size, num_readout]\n",
    "          selected_features: the features fed to the readout (for logging)\n",
    "          avg_spk_rate_input: average spiking rate of the input neuron over the sample and time steps\n",
    "          avg_spk_rate_reservoir: average spiking rate of the reservoir neurons (all) over time\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        input_mem = torch.zeros(batch_size, 1, device=device)\n",
    "        reservoir_mem = torch.zeros(batch_size, self.reservoir_fc.out_features, device=device)\n",
    "        reservoir_spk = torch.zeros(batch_size, self.reservoir_fc.out_features, device=device)\n",
    "        \n",
    "        # For logging spiking activity we accumulate spike counts and record reservoir Vmem at each time step\n",
    "        total_input_spikes = torch.zeros(batch_size, device=device)\n",
    "        total_reservoir_spikes = torch.zeros(batch_size, device=device)\n",
    "        reservoir_mem_record = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            x_t = x[:, t, :]\n",
    "            input_current = self.input_fc(x_t)\n",
    "            input_spk, input_mem = self.input_lif(input_current, input_mem)\n",
    "            total_input_spikes += input_spk.squeeze(1)\n",
    "            \n",
    "            # Map the input spike to reservoir dimension.\n",
    "            reservoir_current = self.reservoir_fc(input_spk)\n",
    "            reservoir_spk, reservoir_mem = self.reservoir_lif(reservoir_current, reservoir_spk, reservoir_mem)\n",
    "            total_reservoir_spikes += reservoir_spk.sum(dim=1)\n",
    "            reservoir_mem_record.append(reservoir_mem)\n",
    "        \n",
    "        # averaging the reservoir membrane potentials over time\n",
    "        reservoir_mem_avg = torch.stack(reservoir_mem_record, dim=0).mean(dim=0)  # [batch_size, reservoir_size]\n",
    "        # select a fixed subset of reservoir neurons.\n",
    "        selected_features = reservoir_mem_avg[:, self.readout_indices]  # [batch_size, num_connected]\n",
    "        # pass through the trainable readout linear layer.\n",
    "        logits = self.readout_fc(selected_features)  # [batch_size, num_readout]\n",
    "        \n",
    "        # computing average spiking rates (per neuron, per time step).\n",
    "        spk_rate_input = (total_input_spikes / time_steps).mean()\n",
    "        spk_rate_reservoir = (total_reservoir_spikes / time_steps).mean()\n",
    "        \n",
    "        return logits, selected_features, spk_rate_input, spk_rate_reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, writer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _, spk_rate_input, spk_rate_reservoir = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Train/Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/SpikingRate_Input', spk_rate_input.item(), epoch)\n",
    "    writer.add_scalar('Train/SpikingRate_Reservoir', spk_rate_reservoir.item(), epoch)\n",
    "\n",
    "    for name, param in model.readout_fc.named_parameters():\n",
    "        writer.add_histogram(f'Weights/Readout_{name}', param, epoch)\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    spk_rate_inputs = []\n",
    "    spk_rate_reservoirs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits, _, spk_rate_input, spk_rate_reservoir = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            spk_rate_inputs.append(spk_rate_input.item())\n",
    "            spk_rate_reservoirs.append(spk_rate_reservoir.item())\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    # for classification we take the argmax since we have 2 outputs\n",
    "    preds = torch.argmax(all_logits, dim=1)\n",
    "    avg_spk_rate_input = np.mean(spk_rate_inputs)\n",
    "    avg_spk_rate_reservoir = np.mean(spk_rate_reservoirs)\n",
    "    return all_labels.numpy(), preds.numpy(), avg_loss, {'input': avg_spk_rate_input, 'reservoir': avg_spk_rate_reservoir}\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='binary')\n",
    "    rec = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    # REMEMBER: for ROC-AUC, if only the predictions are available (not the probabilities) this is approximate.\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, prec, rec, f1, roc_auc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, epoch):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix (Epoch {epoch})')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Abnormal', 'Normal'])\n",
    "    plt.yticks(tick_marks, ['Abnormal', 'Normal'])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    reservoir_size = trial.suggest_int(\"reservoir_size\", 20, 100)\n",
    "    num_connected = trial.suggest_int(\"num_connected\", 10, reservoir_size)\n",
    "    spectral_radius = trial.suggest_float(\"spectral_radius\", 0.5, 1.5)\n",
    "    beta_input = trial.suggest_float(\"beta_input\", 0.7, 0.99)\n",
    "    beta_reservoir = trial.suggest_float(\"beta_reservoir\", 0.7, 0.99)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.5, 1.5)\n",
    "    sparsity_percentage = trial.suggest_float(\"sparsity_percentage\", 0, 50)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    \n",
    "    # For file paths and other parameters we assume they come from a global dict (gs).\n",
    "    global gs\n",
    "    train_dataset = TSVData(gs['train_tsv'])\n",
    "    test_dataset = TSVData(gs['test_tsv'])\n",
    "    \n",
    "    # WeightedRandomSampler for training data, because the classes are imbalanced so we need to upsample the minority class\n",
    "    labels = train_dataset.labels\n",
    "    class_counts = np.bincount(labels)\n",
    "    sample_weights = [1.0 / class_counts[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=gs['batch_size'], sampler=sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=gs['batch_size'], shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SpikingReservoir(\n",
    "        input_size=1,\n",
    "        reservoir_size=reservoir_size,\n",
    "        num_readout=2,\n",
    "        num_connected=num_connected,\n",
    "        spectral_radius=spectral_radius,\n",
    "        beta_input=beta_input,\n",
    "        beta_reservoir=beta_reservoir,\n",
    "        threshold=threshold,\n",
    "        sparsity_percentage=sparsity_percentage,\n",
    "        spike_grad=None,\n",
    "        reset_mechanism='zero',\n",
    "        reset_delay=0\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.readout_fc.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Run for a small number of epochs. NOTE: I have to remember to change this 5 or put a hyperparameter. or use gs['epochs_search']\n",
    "    for epoch in range(5):\n",
    "        train_model(model, train_loader, criterion, optimizer, device, SummaryWriter(log_dir='./temp_runs'), epoch)\n",
    "    \n",
    "    # Evaluate on test set.\n",
    "    y_true, y_pred, _, _ = test_model(model, test_loader, criterion, device)\n",
    "    acc, _, _, _, _, _ = evaluate_metrics(y_true, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main() definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(search_space):\n",
    "    # global search space\n",
    "    global gs\n",
    "    gs = search_space \n",
    "    \n",
    "    seed = search_space.get('seed', 42)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    train_dataset = TSVData(search_space['train_tsv'])\n",
    "    test_dataset = TSVData(search_space['test_tsv'])\n",
    "    \n",
    "    # WeightedRandomSampler for oversampling the minority class.\n",
    "    labels = train_dataset.labels\n",
    "    class_counts = np.bincount(labels)\n",
    "    sample_weights = [1.0 / class_counts[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=search_space['batch_size'], sampler=sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=search_space['batch_size'], shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # with Optuna search, running a short hyperparameter search first\n",
    "    if search_space.get('use_optuna', False):\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=search_space.get('n_trials', 20))\n",
    "        best_params = study.best_params\n",
    "        print(\"Best trial:\", study.best_trial)\n",
    "        print(\"Best params:\", best_params)\n",
    "        # Update the search_space with the best found parameters.\n",
    "        search_space.update(best_params)\n",
    "    \n",
    "    # then build the final model with hyperparameters from search_space\n",
    "    model = SpikingReservoir(\n",
    "        input_size=1,\n",
    "        reservoir_size=search_space['reservoir_size'],\n",
    "        num_readout=2,\n",
    "        num_connected=search_space['num_connected'],\n",
    "        spectral_radius=search_space['spectral_radius'],\n",
    "        beta_input=search_space['beta_input'],\n",
    "        beta_reservoir=search_space['beta_reservoir'],\n",
    "        threshold=search_space['threshold'],\n",
    "        sparsity_percentage=search_space['sparsity_percentage'],\n",
    "        spike_grad=None,\n",
    "        reset_mechanism='zero',\n",
    "        reset_delay=0\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Only train the readout layer.\n",
    "    optimizer = torch.optim.Adam(model.readout_fc.parameters(), lr=search_space['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Set up TensorBoard logging.\n",
    "    writer = SummaryWriter(log_dir=search_space.get('tensorboard_log_dir', './runs'))\n",
    "    \n",
    "    # Final training (using many more epochs than the search)\n",
    "    num_epochs = search_space['epochs_final']\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device, writer, epoch)\n",
    "        y_true, y_pred, test_loss, spk_rates = test_model(model, test_loader, criterion, device)\n",
    "        acc, prec, rec, f1, roc_auc, cm = evaluate_metrics(y_true, y_pred)\n",
    "        \n",
    "        writer.add_scalar('Test/Accuracy', acc, epoch)\n",
    "        writer.add_scalar('Test/Precision', prec, epoch)\n",
    "        writer.add_scalar('Test/Recall', rec, epoch)\n",
    "        writer.add_scalar('Test/F1', f1, epoch)\n",
    "        writer.add_scalar('Test/ROC_AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Test/Loss', test_loss, epoch)\n",
    "        writer.add_scalar('Spiking/InputRate', spk_rates['input'], epoch)\n",
    "        writer.add_scalar('Spiking/ReservoirRate', spk_rates['reservoir'], epoch)\n",
    "        \n",
    "        # Also log the confusion matrix as an image.\n",
    "        fig = plot_confusion_matrix(cm, epoch)\n",
    "        writer.add_figure('Confusion_Matrix', fig, epoch)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f} | Test Loss={test_loss:.4f} | Acc={acc:.4f} | Prec={prec:.4f} | Rec={rec:.4f} | F1={f1:.4f} | ROC_AUC={roc_auc:.4f}\")\n",
    "    \n",
    "    # Save the best model.\n",
    "    save_path = os.path.join(search_space.get('save_dir', '.'), 'best_model.pth')\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main() usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    search_space = {\n",
    "        # Data paths (change these to your actual TSV file locations)\n",
    "        'train_tsv': '/Users/mikel/Documents/GitHub/polimikel/data/UCR_dataset/Wafer/Wafer_TRAIN.tsv',\n",
    "        'test_tsv': '/Users/mikel/Documents/GitHub/polimikel/data/UCR_dataset/Wafer/Wafer_TEST.tsv',\n",
    "        \n",
    "        'reservoir_size': 100,      \n",
    "        'num_connected': 50,        \n",
    "        'spectral_radius': 0.9,    \n",
    "        'beta_input': 0.9,\n",
    "        'beta_reservoir': 0.9,\n",
    "        'threshold': 1.0,\n",
    "        'sparsity_percentage': 10, \n",
    "        \n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'epochs_search': 5,        \n",
    "        'epochs_final': 20,         \n",
    "        \n",
    "        'seed': 42,\n",
    "        \n",
    "        'tensorboard_log_dir': './runs/experiment1',\n",
    "        \n",
    "        'save_dir': './saved_models',\n",
    "        \n",
    "        # For Optuna:\n",
    "        'use_optuna': True,         # set to False if we want to skip hyperparameter search\n",
    "        'n_trials': 20\n",
    "    }\n",
    "    \n",
    "    # Create save directory if it does not exist.\n",
    "    os.makedirs(search_space['save_dir'], exist_ok=True)\n",
    "    \n",
    "    main(search_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroVecio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
