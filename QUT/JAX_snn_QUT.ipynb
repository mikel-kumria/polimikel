{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Spiking Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from jax import jit, grad, value_and_grad\n",
    "import optax # JAX optimizers\n",
    "import functools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0) # PRNGKey is a pseudo-random number generator key, 0 is the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Model parameters\n",
    "        self.input_size = 16\n",
    "        self.hidden_size = 24\n",
    "        self.output_size = 4\n",
    "\n",
    "        # Training parameters\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.num_epochs = 100\n",
    "\n",
    "        # Data parameters\n",
    "        self.weight_1_mean = 1.0\n",
    "        self.weight_2_mean = 0.5\n",
    "        self.weight_3_mean = 0.5\n",
    "        self.weight_4_mean = 0.5\n",
    "        self.weight_1_std = 0.8\n",
    "        self.weight_2_std = 0.5\n",
    "        self.weight_3_std = 0.5\n",
    "        self.weight_4_std = 0.5\n",
    "\n",
    "        self.neuron_params = {\n",
    "            'tau_1' : jnp.array([2,2,2,2,2,2,2,2,2,2,2,2,4,4,4,4,4,4,4,4,4,4,4,4]),\n",
    "            'tau_2' : jnp.array([2,2,2,2,2,2,4,4,4,4,4,4,8,8,8,8,8,8,16,16,16,16,16,16]),\n",
    "            'tau_3' : jnp.array([2,2,2,4,4,4,8,8,8,16,16,16,32,32,32,64,64,64,128,128,128,256,256,256]),\n",
    "            'tau_4' : jnp.array([2,2,2,2]),\n",
    "            'V_reset' : 0.0,\n",
    "            'V_th' : 1.0,\n",
    "        }\n",
    "\n",
    "config = Config()\n",
    "optimizer = optax.adamax(learning_rate=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(key, config):\n",
    "    keys = jax.random.split(key, num=4)\n",
    "    weight_params = {\n",
    "        'weight_1': jax.random.normal(keys[0], (config.input_size, config.hidden_size)) * config.weight_1_std + config.weight_1_mean,\n",
    "        'weight_2': jax.random.normal(keys[1], (config.hidden_size, config.hidden_size)) * config.weight_2_std + config.weight_2_mean,\n",
    "        'weight_3': jax.random.normal(keys[2], (config.hidden_size, config.hidden_size)) * config.weight_3_std + config.weight_3_mean,\n",
    "        'weight_4': jax.random.normal(keys[3], (config.hidden_size, config.output_size)) * config.weight_4_std + config.weight_4_mean,\n",
    "    }\n",
    "    return weight_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    " - load dataset\n",
    " - preprocess\n",
    " - split data into training, validation, test\n",
    " - create batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from directory: /home/kumria/Documents/Offline_Datasets/4_one_second_samples\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_directory(data_dir):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    labels = ['CAR', 'STREET', 'HOME', 'CAFE']\n",
    "    label_to_index = {label: index for index, label in enumerate(labels)}\n",
    "    \n",
    "    print(f\"Loading data from directory: {data_dir}\")\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            data = df.iloc[:, :-1].values  # First 16 columns\n",
    "            label_values = df['Label'].unique()\n",
    "            \n",
    "            if len(label_values) != 1:\n",
    "                raise ValueError(f\"Inconsistent labels in file: {filepath}\")\n",
    "            \n",
    "            label = label_values[0]\n",
    "            label_index = label_to_index.get(label)\n",
    "            if label_index is None:\n",
    "                raise ValueError(f\"Unknown label '{label}' in file: {filepath}\")\n",
    "            \n",
    "            if data.shape != (1000, 16):\n",
    "                raise ValueError(f\"Unexpected data shape in file: {filepath}. Expected (1000, 16), got {data.shape}\")\n",
    "            \n",
    "            data_list.append(data)\n",
    "            labels_list.append(label_index)\n",
    "    \n",
    "    # Stack data and labels\n",
    "    all_data = np.stack(data_list)  # Shape: (num_samples, 1000, 16)\n",
    "    all_labels = np.array(labels_list)  # Shape: (num_samples,)\n",
    "    \n",
    "    print(f\"Total samples loaded: {all_data.shape[0]}\")\n",
    "    return all_data, all_labels\n",
    "\n",
    "\n",
    "dir_test = '/home/kumria/Documents/Offline_Datasets/TEST'\n",
    "dir_complete = '/home/kumria/Documents/Offline_Datasets/4_one_second_samples'\n",
    "\n",
    "data, labels = load_data_from_directory(dir_complete)\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(all_data, all_labels, train_ratio=0.65, val_ratio=0.15, test_ratio=0.2, seed=42):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1\"\n",
    "    num_samples = all_data.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(train_ratio * num_samples)\n",
    "    val_end = train_end + int(val_ratio * num_samples)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "    \n",
    "    train_data = all_data[train_indices]\n",
    "    train_labels = all_labels[train_indices]\n",
    "    val_data = all_data[val_indices]\n",
    "    val_labels = all_labels[val_indices]\n",
    "    test_data = all_data[test_indices]\n",
    "    test_labels = all_labels[test_indices]\n",
    "    \n",
    "    print(f\"Dataset split: Train {len(train_data)}, Validation {len(val_data)}, Test {len(test_data)}\")\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return jax.nn.one_hot(labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_dir = '/home/kumria/Documents/Offline_Datasets/TEST'\n",
    "    all_data, all_labels = load_data_from_directory(data_dir)\n",
    "    \n",
    "    # Split data\n",
    "    (train_data, train_labels), (val_data, val_labels), (test_data, test_labels) = split_data(all_data, all_labels)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    num_classes = 4  # As per my labels: 'CAR', 'STREET', 'HOME', 'CAFE'\n",
    "    train_labels_onehot = one_hot_encode(train_labels, num_classes)\n",
    "    val_labels_onehot = one_hot_encode(val_labels, num_classes)\n",
    "    test_labels_onehot = one_hot_encode(test_labels, num_classes)\n",
    "    \n",
    "    # Convert to JAX arrays\n",
    "    train_data = jnp.array(train_data)\n",
    "    train_labels_onehot = jnp.array(train_labels_onehot)\n",
    "    val_data = jnp.array(val_data)\n",
    "    val_labels_onehot = jnp.array(val_labels_onehot)\n",
    "    test_data = jnp.array(test_data)\n",
    "    test_labels_onehot = jnp.array(test_labels_onehot)\n",
    "    \n",
    "    # Also keep integer labels for accuracy computation\n",
    "    train_labels = jnp.array(train_labels)\n",
    "    val_labels = jnp.array(val_labels)\n",
    "    test_labels = jnp.array(test_labels)\n",
    "    \n",
    "    return (train_data, train_labels_onehot, train_labels), \\\n",
    "           (val_data, val_labels_onehot, val_labels), \\\n",
    "           (test_data, test_labels_onehot, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX does not have DataLoader, so we have to create our own batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LIF Spiking Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_neuron(V_input, input_current, tau, V_th, V_reset):\n",
    "\n",
    "    # update the membrane potential\n",
    "    dV = (input_current - V_input) / tau        # dV/dt = (R*I - V) / tau so we are considering R==1, and dV == dV/dt so we are considering dt==1\n",
    "    V = V_input + dV\n",
    "\n",
    "    # spike generation\n",
    "    spike = (V > V_th).astype(jnp.float32)      # if V > V_th, spike = 1, otherwise spike = 0\n",
    "\n",
    "    # reset the membrane potential after spike\n",
    "    V = jnp.where(spike, V_reset, V)            # if spike = 1, V = V_reset (can reset to either zero or another mechanism), otherwise V_new = V_old (so nothing changes)\n",
    "\n",
    "    return V, spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def SNN_forward(neuron_params, inputs, weight_params):\n",
    "    \n",
    "    batch_size, time_steps, input_size = inputs.shape\n",
    "    hidden_size = neuron_params['tau_1'].shape[0]\n",
    "    output_size = neuron_params['tau_4'].shape[0]\n",
    "    # we are using neuron_params instead of config because JAX jit does not allow to use config in the function\n",
    "\n",
    "\n",
    "    # call the tau\n",
    "    tau_1 = neuron_params['tau_1']\n",
    "    tau_2 = neuron_params['tau_2']\n",
    "    tau_3 = neuron_params['tau_3']\n",
    "    tau_4 = neuron_params['tau_4']\n",
    "\n",
    "    V_th = neuron_params['V_th']\n",
    "    V_reset = neuron_params['V_reset']\n",
    "\n",
    "    # initialize the membrane potentials\n",
    "    V1 = jnp.zeros((batch_size, hidden_size))\n",
    "    V2 = jnp.zeros((batch_size, hidden_size))\n",
    "    V3 = jnp.zeros((batch_size, hidden_size))\n",
    "    V4 = jnp.zeros((batch_size, output_size))\n",
    "\n",
    "    # JAX-OPTIMIZED LOOP\n",
    "    def step(carry, x_t):\n",
    "        V1, V2, V3, V4 = carry # get the membrane potentials from the previous time step (carry-over)\n",
    "\n",
    "        # layer 1\n",
    "        I1 = jnp.dot(x_t, weight_params['weight_1'])\n",
    "        V1_new, spike_1 = lif_neuron(V1, I1, tau_1, V_th, V_reset)\n",
    "\n",
    "        # layer 2\n",
    "        I2 = jnp.dot(spike_1, weight_params['weight_2'])\n",
    "        V2_new, spike_2 = lif_neuron(V2, I2, tau_2, V_th, V_reset)\n",
    "\n",
    "        # layer 3\n",
    "        I3 = jnp.dot(spike_2, weight_params['weight_3'])\n",
    "        V3_new, spike_3 = lif_neuron(V3, I3, tau_3, V_th, V_reset)\n",
    "\n",
    "        # layer 4\n",
    "        I4 = jnp.dot(spike_3, weight_params['weight_4'])\n",
    "        V4_new, spike_4 = lif_neuron(V4, I4, tau_4, 1e7, V_reset)\n",
    "\n",
    "        new_carry = (V1_new, V2_new, V3_new, V4_new)\n",
    "\n",
    "        output = (spike_4, V4_new)\n",
    "\n",
    "        return new_carry, output\n",
    "\n",
    "   \n",
    "    inputs = inputs.transpose(1, 0, 2)  # transpose to change the shape of inputs, to bring time_steps in the first dimension for the scan operation: (time_steps, batch_size, input_size)\n",
    "    _, outputs = jax.lax.scan(step, (V1, V2, V3, V4), inputs)\n",
    "\n",
    "    spikes, membrane_potentials = outputs\n",
    "\n",
    "    return spikes, membrane_potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE: after jax.lax.scan the spikes and membrane_potentials will have shapes (time_steps, batch_size, output_size)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(y_true, y_pred):\n",
    "    return jnp.mean(jnp.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train, Validate, Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train(weights, opt_state, inputs, targets_onehot, neuron_params):\n",
    "    def loss_fn(weights):\n",
    "        _, membrane_potentials = SNN_forward(neuron_params, inputs, weights)\n",
    "        outputs = jnp.sum(membrane_potentials, axis=0)  # Sum over time steps\n",
    "        loss = MSE_loss(targets_onehot, outputs)\n",
    "        return loss, outputs\n",
    "\n",
    "    (loss, outputs), grads = value_and_grad(loss_fn, has_aux=True)(weights) # by using hax_aux=True, we can return the outputs from the loss function (we can return auxiliaty data like outputs, from the loss function)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, weights)\n",
    "    weights = optax.apply_updates(weights, updates)\n",
    "    predictions = jnp.argmax(outputs, axis=-1) # axis=-1 means the last axis\n",
    "\n",
    "    return weights, opt_state, loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(weights, inputs, targets_onehot, targets_int, neuron_params):\n",
    "\n",
    "    # print(\"Targets One-Hot:\", targets_onehot)\n",
    "    # print(\"Targets Integer:\", targets_int)\n",
    "    \n",
    "    _, membrane_potentials = SNN_forward(neuron_params, inputs, weights)\n",
    "    outputs = jnp.sum(membrane_potentials, axis=0)  # Sum over time steps\n",
    "    loss = MSE_loss(targets_onehot, outputs)\n",
    "    predictions = jnp.argmax(outputs, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == targets_int)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(weights, inputs, targets_onehot, targets_int, neuron_params):\n",
    "    loss, accuracy = validate(weights, inputs, targets_onehot, targets_int, neuron_params)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - main -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config()\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    optimizer = optax.adamax(learning_rate=config.learning_rate)\n",
    "    weights = initialize_weights(key, config)\n",
    "    opt_state = optimizer.init(weights)\n",
    "\n",
    "    # Load data\n",
    "    (train_data, train_labels_onehot, train_labels), \\\n",
    "    (validation_data, validation_labels_onehot, validation_labels), \\\n",
    "    (test_data, test_labels_onehot, test_labels) = load_data()\n",
    "\n",
    "    num_epochs = config.num_epochs\n",
    "    batch_size = config.batch_size\n",
    "    num_batches = train_data.shape[0] // batch_size\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # shuffle data\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, train_data.shape[0]) # generate a random permutation of the numbers from 0 to num_samples of the training data\n",
    "        train_data_shuffled = train_data[perm]\n",
    "        train_labels_shuffled = train_labels[perm]\n",
    "        train_labels_onehot_shuffled = train_labels_onehot[perm]\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            # get the batch data\n",
    "            batch_data = train_data_shuffled[i * batch_size : (i+1) * batch_size] # the i*batch_size:(i+1)*batch_size does the batching by taking the samples FROM \"i*batch_size\" TO \"(i+1)*batch_size\"\n",
    "            batch_labels = train_labels_shuffled[i * batch_size : (i+1) * batch_size]\n",
    "            batch_labels_onehot = train_labels_onehot_shuffled[i * batch_size : (i+1) * batch_size]\n",
    "\n",
    "            # update the parameters\n",
    "            weights, opt_state, loss, predictions = train(weights, opt_state, batch_data, batch_labels_onehot, config.neuron_params)\n",
    "            epoch_loss += loss\n",
    "            accuracy = jnp.mean(predictions == batch_labels)\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "        # average metrics over batches\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_accuracy /= num_batches\n",
    "\n",
    "        # validate the model\n",
    "        validation_loss, validation_accuracy = validate(weights, validation_data, validation_labels_onehot, validation_labels, config.neuron_params)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}   ||   Loss: {epoch_loss:.4f}   ||   Accuracy: {100*epoch_accuracy:.4f}%   ||   Validation Loss: {validation_loss:.4f}   ||   Validation Accuracy: {100*validation_accuracy:.4f}%\")\n",
    "\n",
    "    \n",
    "    # test the model\n",
    "    test_loss, test_accuracy = test(weights, test_data, test_labels_onehot, test_labels, config.neuron_params) # test the model on the test data\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}   ||   Test Accuracy: {100*test_accuracy:.4f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroNova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
