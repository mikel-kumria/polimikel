{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "import optuna\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from WOR_dataset import Wave_Order_Dataset, split_dataset, get_dataloaders\n",
    "from WOR_plot import (plot_wave, plot_accuracies, plot_loss_curve, plot_metrics, \n",
    "                      plot_equal_prediction_values, plot_beta_values, plot_tau_values, \n",
    "                      plot_layer_weights, plot_spike_counts, plot_snn_spikes, \n",
    "                      plot_membrane_potentials, plot_threshold_potentials)\n",
    "from WOR_train_val_test import train_model, validate_model, test_model\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"seed\": 42,\n",
    "    \"std_dev\": 0.0,\n",
    "    \"num_samples\": 1000,\n",
    "    \"train_ratio\": 0.7,\n",
    "    \"validation_ratio\": 0.2,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"freq_min\": 2,\n",
    "    \"freq_max\": 2,\n",
    "    \"amp_min\": 1.1,\n",
    "    \"amp_max\": 1.1,\n",
    "    \"offset\": 1.1,\n",
    "    \"sample_rate\": 40,       # NOTE: Change this along with freq_max to avoid aliasing.\n",
    "    \"duration\": 3,\n",
    "    \"input_size\": 1,\n",
    "    \"output_size\": 2,\n",
    "    \"optimizer_betas\": (0.99, 0.999),\n",
    "    \"scheduler_step_size\": 30,\n",
    "    \"scheduler_gamma\": 0.5,\n",
    "    \"L1_lambda\": 0.001,\n",
    "    \"N_hidden_weights_gaussian\": 30,\n",
    "    \"N_output_weights_gaussian\": 1,\n",
    "    \"hidden_reset_mechanism\": 'zero',\n",
    "    \"output_reset_mechanism\": 'zero',\n",
    "    \"weights_hidden_min_clamped\": 0.0,\n",
    "    \"weights_hidden_max_clamped\": 2.0,\n",
    "    \"weights_output_min_clamped\": 0.0,\n",
    "    \"weights_output_max_clamped\": 2.0,\n",
    "    \"N_hidden_tau\": 0.1,\n",
    "    \"N_output_tau\": 0.1,\n",
    "    \"learn_threshold_hidden\": True,\n",
    "    \"learn_threshold_output\": True,\n",
    "    \"learn_beta_hidden\": True,\n",
    "    \"learn_beta_output\": True,\n",
    "    \"phase1\": \"random_uniform_0_to_2pi\",\n",
    "    \"phase2\": \"random_uniform_0_to_2pi\",\n",
    "    # These parameters will be tuned by HPO:\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"hidden_size\": 70,\n",
    "    \"threshold_hidden_min\": 1.0,\n",
    "    \"threshold_hidden_max\": 1.1,\n",
    "    \"penalty_weight\": 0.0,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOR Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave_Order_Recognition_SNN(nn.Module):\n",
    "    def __init__(self, hyperparams):\n",
    "        super().__init__()\n",
    "        hidden_size = hyperparams[\"hidden_size\"]\n",
    "        output_size = hyperparams[\"output_size\"]\n",
    "        input_size = hyperparams[\"input_size\"]\n",
    "        sample_rate = hyperparams[\"sample_rate\"]\n",
    "        duration = hyperparams[\"duration\"]\n",
    "        deltaT = 1 / sample_rate\n",
    "        self.num_steps = int(duration / deltaT)\n",
    "        \n",
    "        # tau and beta values for hidden and output layers\n",
    "        tau_hidden = torch.Tensor(hidden_size).uniform_(\n",
    "            hyperparams[\"N_hidden_tau\"] * hyperparams[\"freq_min\"],\n",
    "            hyperparams[\"N_hidden_tau\"] * hyperparams[\"freq_max\"]\n",
    "        )\n",
    "        tau_output = torch.Tensor(output_size).uniform_(\n",
    "            hyperparams[\"N_output_tau\"] * hyperparams[\"freq_min\"],\n",
    "            hyperparams[\"N_output_tau\"] * hyperparams[\"freq_max\"]\n",
    "        )\n",
    "        beta_hidden = torch.exp(-deltaT / tau_hidden)\n",
    "        beta_output = torch.exp(-deltaT / tau_output)\n",
    "        \n",
    "        # thresholds: tuned for the hidden layer, fixed range for output\n",
    "        threshold_hidden = np.random.uniform(\n",
    "            hyperparams[\"threshold_hidden_min\"],\n",
    "            hyperparams[\"threshold_hidden_max\"],\n",
    "            hidden_size\n",
    "        )\n",
    "        threshold_output = np.random.uniform(1.0, 1.1, output_size)\n",
    "        \n",
    "        # Gaussian initialization parameters for weights.\n",
    "        N_hidden_weights_gaussian = hyperparams[\"N_hidden_weights_gaussian\"]\n",
    "        N_output_weights_gaussian = hyperparams[\"N_output_weights_gaussian\"]\n",
    "        N_hidden_weights_std = np.sqrt(N_hidden_weights_gaussian)\n",
    "        N_output_weights_std = np.sqrt(N_output_weights_gaussian)\n",
    "        gaussian_mean_hidden_weights = N_hidden_weights_gaussian / sample_rate\n",
    "        gaussian_std_hidden_weights = N_hidden_weights_std / sample_rate\n",
    "        gaussian_mean_output_weights = N_output_weights_gaussian / (sample_rate * hidden_size)\n",
    "        gaussian_std_output_weights = N_output_weights_std / (sample_rate * hidden_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=beta_hidden,\n",
    "            threshold=threshold_hidden,\n",
    "            learn_beta=hyperparams[\"learn_beta_hidden\"],\n",
    "            learn_threshold=hyperparams[\"learn_threshold_hidden\"],\n",
    "            reset_mechanism=hyperparams[\"hidden_reset_mechanism\"],\n",
    "            reset_delay=False\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=beta_output,\n",
    "            threshold=threshold_output,\n",
    "            learn_beta=hyperparams[\"learn_beta_output\"],\n",
    "            learn_threshold=hyperparams[\"learn_threshold_output\"],\n",
    "            reset_mechanism=hyperparams[\"output_reset_mechanism\"],\n",
    "            reset_delay=False\n",
    "        )\n",
    "        self._initialize_weights(\n",
    "            gaussian_mean_hidden_weights, gaussian_std_hidden_weights,\n",
    "            gaussian_mean_output_weights, gaussian_std_output_weights\n",
    "        )\n",
    "    \n",
    "    def _initialize_weights(self, mean_hidden, std_hidden, mean_output, std_output):\n",
    "        nn.init.normal_(self.fc1.weight, mean=mean_hidden, std=std_hidden)\n",
    "        nn.init.normal_(self.fc2.weight, mean=mean_output, std=std_output)\n",
    "    \n",
    "    def forward(self, x, mem1=None, mem2=None):\n",
    "        batch_size = x.size(0)\n",
    "        if mem1 is None:\n",
    "            mem1 = torch.zeros(batch_size, self.fc1.out_features, device=x.device)\n",
    "        if mem2 is None:\n",
    "            mem2 = torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "        \n",
    "        spk1_rec, mem1_rec, spk2_rec, mem2_rec = [], [], [], []\n",
    "        hidden_spike_count = 0\n",
    "        output_spike_count = 0\n",
    "        \n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step].unsqueeze(1))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            \n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "            \n",
    "            hidden_spike_count += spk1.sum().item()\n",
    "            output_spike_count += spk2.sum().item()\n",
    "        \n",
    "        return (torch.stack(spk1_rec, dim=0),\n",
    "                torch.stack(mem1_rec, dim=0),\n",
    "                torch.stack(spk2_rec, dim=0),\n",
    "                torch.stack(mem2_rec, dim=0),\n",
    "                hidden_spike_count,\n",
    "                output_spike_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Objective for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Override a few hyperparameters using trial suggestions.\n",
    "    hyperparams[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)\n",
    "    hyperparams[\"threshold_hidden_min\"] = trial.suggest_float(\"threshold_hidden_min\", 0.8, 1.2)\n",
    "    hyperparams[\"threshold_hidden_max\"] = trial.suggest_float(\"threshold_hidden_max\", hyperparams[\"threshold_hidden_min\"] + 0.1, 2.0)\n",
    "    \n",
    "    # Create dataset and dataloaders.\n",
    "    dataset = Wave_Order_Dataset(\n",
    "        hyperparams[\"num_samples\"],\n",
    "        hyperparams[\"sample_rate\"],\n",
    "        hyperparams[\"duration\"],\n",
    "        hyperparams[\"freq_min\"],\n",
    "        hyperparams[\"freq_max\"],\n",
    "        hyperparams[\"amp_min\"],\n",
    "        hyperparams[\"amp_max\"],\n",
    "        hyperparams[\"std_dev\"],\n",
    "        hyperparams[\"offset\"]\n",
    "    )\n",
    "    train_dataset, validation_dataset, _ = split_dataset(\n",
    "        dataset,\n",
    "        hyperparams[\"train_ratio\"],\n",
    "        hyperparams[\"validation_ratio\"],\n",
    "        hyperparams[\"test_ratio\"]\n",
    "    )\n",
    "    train_loader, validation_loader, _ = get_dataloaders(\n",
    "        train_dataset, validation_dataset, [], hyperparams[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Instantiate model, loss, optimizer, and scheduler.\n",
    "    model = Wave_Order_Recognition_SNN(hyperparams).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "                                   betas=hyperparams[\"optimizer_betas\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=hyperparams[\"scheduler_step_size\"],\n",
    "                                                gamma=hyperparams[\"scheduler_gamma\"])\n",
    "    \n",
    "    # Run a short training loop for HPO (using fewer epochs for speed).\n",
    "    num_epochs = hyperparams[\"num_epochs\"] // 10\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, _, _, _, _ = train_model(model, train_loader, criterion, optimizer,\n",
    "                                             epoch, num_epochs, hyperparams[\"batch_size\"],\n",
    "                                             hyperparams[\"hidden_size\"], hyperparams[\"output_size\"],\n",
    "                                             hyperparams[\"weights_hidden_min_clamped\"],\n",
    "                                             hyperparams[\"weights_hidden_max_clamped\"],\n",
    "                                             hyperparams[\"weights_output_min_clamped\"],\n",
    "                                             hyperparams[\"weights_output_max_clamped\"],\n",
    "                                             hyperparams[\"penalty_weight\"],\n",
    "                                             hyperparams[\"L1_lambda\"], device)\n",
    "        val_accuracy, val_loss = validate_model(model, validation_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Trial {trial.number} Epoch {epoch}: Val Acc {val_accuracy:.2f}% Loss {val_loss:.4f}\")\n",
    "    \n",
    "    # validation accuracy (metric to maximize)\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run (after HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(hyperparams):\n",
    "    set_seed(hyperparams[\"seed\"])\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # folder for saving outputs\n",
    "    now = datetime.now()\n",
    "    output_folder = os.path.join(os.getcwd(), \"thesis_simulations\", now.strftime(\"run_%Y%m%d_%H%M%S\"))\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create dataset and dataloaders\n",
    "    dataset = Wave_Order_Dataset(\n",
    "        hyperparams[\"num_samples\"],\n",
    "        hyperparams[\"sample_rate\"],\n",
    "        hyperparams[\"duration\"],\n",
    "        hyperparams[\"freq_min\"],\n",
    "        hyperparams[\"freq_max\"],\n",
    "        hyperparams[\"amp_min\"],\n",
    "        hyperparams[\"amp_max\"],\n",
    "        hyperparams[\"std_dev\"],\n",
    "        hyperparams[\"offset\"]\n",
    "    )\n",
    "    train_dataset, validation_dataset, test_dataset = split_dataset(\n",
    "        dataset,\n",
    "        hyperparams[\"train_ratio\"],\n",
    "        hyperparams[\"validation_ratio\"],\n",
    "        hyperparams[\"test_ratio\"]\n",
    "    )\n",
    "    train_loader, validation_loader, test_loader = get_dataloaders(\n",
    "        train_dataset, validation_dataset, test_dataset, hyperparams[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # model, loss, optimizer, and scheduler.\n",
    "    model = Wave_Order_Recognition_SNN(hyperparams).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "                                   betas=hyperparams[\"optimizer_betas\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=hyperparams[\"scheduler_step_size\"],\n",
    "                                                gamma=hyperparams[\"scheduler_gamma\"])\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = hyperparams[\"num_epochs\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, _, _, _, _ = train_model(model, train_loader, criterion, optimizer,\n",
    "                                             epoch, num_epochs, hyperparams[\"batch_size\"],\n",
    "                                             hyperparams[\"hidden_size\"], hyperparams[\"output_size\"],\n",
    "                                             hyperparams[\"weights_hidden_min_clamped\"],\n",
    "                                             hyperparams[\"weights_hidden_max_clamped\"],\n",
    "                                             hyperparams[\"weights_output_min_clamped\"],\n",
    "                                             hyperparams[\"weights_output_max_clamped\"],\n",
    "                                             hyperparams[\"penalty_weight\"],\n",
    "                                             hyperparams[\"L1_lambda\"], device)\n",
    "        val_accuracy, val_loss = validate_model(model, validation_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Acc = {val_accuracy:.2f}% | Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_metrics = test_model(model, test_loader, criterion, device)\n",
    "    print(\"Test Metrics:\")\n",
    "    print(test_metrics)\n",
    "    \n",
    "    # Save the final model\n",
    "    model_save_path = os.path.join(output_folder, \"final_model.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Return a dictionary with useful outputs.\n",
    "    return {\n",
    "        \"output_folder\": output_folder,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"validation_loader\": validation_loader,\n",
    "        \"test_loader\": test_loader,\n",
    "        \"model\": model,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        #\"deltaT\": 1 / hyperparams[\"sample_rate\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'mode' Selection and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:11,627] A new study created in memory with name: no-name-14a92d30-b320-40ab-82d9-8bac428edefd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 0: Val Acc 47.62% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:15,665] Trial 0 finished with value: 50.476190476190474 and parameters: {'learning_rate': 0.0006063220466440857, 'threshold_hidden_min': 1.1549939282394002, 'threshold_hidden_max': 1.668972803525267}. Best is trial 0 with value: 50.476190476190474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Epoch 0: Val Acc 50.52% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:18,810] Trial 1 finished with value: 51.54639175257732 and parameters: {'learning_rate': 0.00016240583045087637, 'threshold_hidden_min': 0.829079692120461, 'threshold_hidden_max': 1.4698093570537463}. Best is trial 1 with value: 51.54639175257732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Epoch 0: Val Acc 42.11% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:21,902] Trial 2 finished with value: 56.8421052631579 and parameters: {'learning_rate': 0.008762316412516962, 'threshold_hidden_min': 0.9280475465880023, 'threshold_hidden_max': 1.8567201498310255}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Epoch 0: Val Acc 47.47% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:25,017] Trial 3 finished with value: 47.474747474747474 and parameters: {'learning_rate': 0.023637447009193763, 'threshold_hidden_min': 0.8597470196894832, 'threshold_hidden_max': 0.9603486480830844}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Epoch 0: Val Acc 50.50% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:28,550] Trial 4 finished with value: 42.57425742574257 and parameters: {'learning_rate': 0.009174011771649733, 'threshold_hidden_min': 1.0983704914118737, 'threshold_hidden_max': 1.6584158234076558}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 0: Val Acc 55.10% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:31,715] Trial 5 finished with value: 48.97959183673469 and parameters: {'learning_rate': 0.06302987116092815, 'threshold_hidden_min': 1.08741923637226, 'threshold_hidden_max': 1.8512458816691852}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Epoch 0: Val Acc 47.42% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:35,014] Trial 6 finished with value: 44.329896907216494 and parameters: {'learning_rate': 0.018371435314968373, 'threshold_hidden_min': 0.9356750170854905, 'threshold_hidden_max': 1.9991098258323243}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Epoch 0: Val Acc 49.00% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:38,258] Trial 7 finished with value: 51.0 and parameters: {'learning_rate': 0.0003791190668625142, 'threshold_hidden_min': 0.8888478344496423, 'threshold_hidden_max': 1.6290291333726536}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Epoch 0: Val Acc 52.00% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:41,488] Trial 8 finished with value: 54.0 and parameters: {'learning_rate': 0.007896062006436597, 'threshold_hidden_min': 0.9013967165843327, 'threshold_hidden_max': 1.9426632892972697}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Epoch 0: Val Acc 52.13% Loss 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 01:14:44,508] Trial 9 finished with value: 43.61702127659574 and parameters: {'learning_rate': 0.0036577622496122616, 'threshold_hidden_min': 0.8690648074588009, 'threshold_hidden_max': 1.0919515328299072}. Best is trial 2 with value: 56.8421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 56.8421052631579\n",
      "  Params:\n",
      "    learning_rate: 0.008762316412516962\n",
      "    threshold_hidden_min: 0.9280475465880023\n",
      "    threshold_hidden_max: 1.8567201498310255\n",
      "Best hyperparameters saved to best_hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "# Set mode = \"hpo\" for hyperparameter optimization, or \"train\" for a full training run.\n",
    "mode = \"hpo\"  # change to \"hpo\" to run hyperparameter optimization\n",
    "\n",
    "if mode == \"hpo\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)  # adjust n_trials as needed\n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best trial:\")\n",
    "    print(f\"  Value: {best_trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    # Save best hyperparameters to a JSON file.\n",
    "    with open(\"best_hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(best_trial.params, f, indent=4)\n",
    "    print(\"Best hyperparameters saved to best_hyperparameters.json\")\n",
    "    \n",
    "elif mode == \"train\":\n",
    "    # Load best hyperparameters (ensure you have run an HPO run first so that the JSON exists).\n",
    "    with open(\"best_hyperparameters.json\", \"r\") as f:\n",
    "        best_tuned = json.load(f)\n",
    "    # Merge tuned hyperparameters with the fixed ones.\n",
    "    hyperparams = {**hyperparams, **best_tuned}\n",
    "    results = run_training(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_wave(\u001b[43mtrain_loader\u001b[49m, save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(unique_folder_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwave_samples.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "plot_wave(train_loader, save_path=os.path.join(unique_folder_name, 'wave_samples.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(num_epochs, test_accuracies, validation_accuracies, os.path.join(unique_folder_name, 'accuracy_plot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss_hist, test_loss_hist, num_epochs, os.path.join(unique_folder_name, 'loss_curve'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(test_metrics,os.path.join(unique_folder_name, 'evaluation_plots'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Prediction Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_equal_prediction_values(equal_prediction_values, num_epochs, os.path.join(unique_folder_name, 'equal_prediction_values'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_values(beta1_values, num_epochs, os.path.join(unique_folder_name, 'beta_hidden_layer'), layer_name='Hidden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_values(beta2_values, num_epochs, os.path.join(unique_folder_name, 'beta_output_layer'), layer_name='Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tau Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tau_values(beta1_values, num_epochs, deltaT, os.path.join(unique_folder_name, 'tau_hidden_layer'), layer_name='Hidden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tau Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tau_values(beta2_values, num_epochs, deltaT, os.path.join(unique_folder_name, 'tau_output_layer'), layer_name='Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_weights(weights_hidden_layer, num_epochs, os.path.join(unique_folder_name, 'weights_hidden_layer'), layer_name='Hidden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_weights(weights_output_layer, num_epochs, os.path.join(unique_folder_name, 'weights_output_layer'), layer_name='Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Count vs Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spike_counts(hidden_spike_count, output_spike_count, output_spike_counts_neuron0, output_spike_counts_neuron1, num_epochs, os.path.join(unique_folder_name, 'spike_counts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spikes Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snn_spikes(model, test_loader, device, os.path.join(unique_folder_name, 'hidden_layer_spikes'), layer_name='Hidden', layer_size=hidden_size, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spikes Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snn_spikes(model, test_loader, device, os.path.join(unique_folder_name, 'output_layer_spikes'), layer_name='Output', layer_size=output_size, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vmem Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_membrane_potentials(model, test_loader, device, 'Hidden', hidden_size, num_steps, os.path.join(unique_folder_name, 'hidden_membrane_potentials'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vmem Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_membrane_potentials(model, test_loader, device, 'Output', output_size, num_steps, os.path.join(unique_folder_name, 'output_membrane_potentials'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_threshold_potentials(threshold_hidden_layer, num_epochs, os.path.join(unique_folder_name, 'threshold_hidden_layer'), 'Hidden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_threshold_potentials(threshold_output_layer, num_epochs, os.path.join(unique_folder_name, 'threshold_output_layer'), 'Output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroVecio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
