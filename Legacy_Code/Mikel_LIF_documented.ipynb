{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nni\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from snntorch import surrogate\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "\n",
    "from QUT_DataLoader import QUTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and Constants\n",
    "input_size = 16\n",
    "hidden_size = 24\n",
    "output_size = 4\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "scheduler_step_size = 3\n",
    "scheduler_gamma = 0.5 # 0.5 means halving the learning rate every step_size epochs\n",
    "\n",
    "output_threshold = 1e7\n",
    "hidden_threshold = 1\n",
    "\n",
    "num_workers = max(1, os.cpu_count() - 1)\n",
    "hidden_reset_mechanism = 'subtract'\n",
    "output_reset_mechanism = 'none'\n",
    "\n",
    "# Vmem shift to be used for MSELoss\n",
    "Vmem_shift_for_MSELoss = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def get_nni_params() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve hyperparameters for model training, integrating with NNI (Neural Network Intelligence) if available.\n",
    "\n",
    "    This function serves as a centralized way to define and update hyperparameters for your model.\n",
    "    It initializes a set of default hyperparameters and, if NNI is being used for hyperparameter tuning,\n",
    "    updates these defaults with the parameters suggested by NNI's tuner.\n",
    "\n",
    "    **Functionality Overview:**\n",
    "\n",
    "    1. **Initialize Default Hyperparameters:**\n",
    "       - A dictionary `params` is created with default hyperparameter values that are used when NNI is not available or not being used.\n",
    "       - Default hyperparameters include:\n",
    "         - `'learning_rate'`: Controls how much to adjust the weights with respect to the loss gradient. Default is `0.01`.\n",
    "         - `'optimizer_betas'`: Beta coefficients for the optimizer (e.g., Adam or Adamax). Default is `(0.9, 0.99)`.\n",
    "         - `'fast_sigmoid_slope'`: Parameter for the slope of the surrogate gradient in spiking neural networks. Default is `10`.\n",
    "\n",
    "    2. **Integrate with NNI for Hyperparameter Tuning:**\n",
    "       - Attempts to retrieve hyperparameters from NNI using `nni.get_next_parameter()`.\n",
    "       - If NNI is being used, it provides new values for hyperparameters that are being tuned.\n",
    "       - The default parameters are updated with these new values, ensuring that the model uses the hyperparameters suggested by NNI.\n",
    "\n",
    "    3. **Handle the Case When NNI Is Not Used:**\n",
    "       - If NNI is not being used, `nni.get_next_parameter()` may return an empty dictionary or raise an exception.\n",
    "       - To handle this, exception handling is included to catch any errors when NNI is not available.\n",
    "       - In such cases, the function retains the default hyperparameters defined at the start.\n",
    "\n",
    "    4. **Return the Final Hyperparameters:**\n",
    "       - The function returns the `params` dictionary, which contains the hyperparameters to be used in model training.\n",
    "\n",
    "    **Line-by-Line Explanation:**\n",
    "\n",
    "    ```python\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer_betas': (0.9, 0.99),\n",
    "        'fast_sigmoid_slope': 10,\n",
    "    }\n",
    "    ```\n",
    "    - **Purpose:** Initialize a dictionary named `params` with default hyperparameters.\n",
    "    - **Details:**\n",
    "      - `'learning_rate'`: Sets the learning rate for the optimizer. A smaller value means the weights are updated more slowly.\n",
    "      - `'optimizer_betas'`: Tuple containing beta coefficients for optimizers like Adam/Adamax, controlling the decay rates of moving averages.\n",
    "      - `'fast_sigmoid_slope'`: Defines the steepness of the surrogate gradient's activation function in spiking neural networks.\n",
    "\n",
    "    ```python\n",
    "    try:\n",
    "        tuner_params = nni.get_next_parameter()\n",
    "        params.update(tuner_params)\n",
    "    except Exception as e:\n",
    "        print(\"NNI is not being used or failed to retrieve parameters. Using default hyperparameters.\")\n",
    "    ```\n",
    "    - **Purpose:** Update the default hyperparameters with those from NNI, if available.\n",
    "    - **Details:**\n",
    "      - `try` block:\n",
    "        - `tuner_params = nni.get_next_parameter()`: Attempts to get the next set of parameters from NNI's tuner.\n",
    "          - If NNI is running, this function returns a dictionary with hyperparameters that NNI wants to test.\n",
    "          - If NNI is not running, this function may raise an exception or return an empty dictionary.\n",
    "        - `params.update(tuner_params)`: Updates the `params` dictionary with the values from `tuner_params`.\n",
    "          - This means any hyperparameters suggested by NNI will overwrite the defaults.\n",
    "      - `except` block:\n",
    "        - Catches any exceptions that occur if NNI is not available.\n",
    "        - Prints a message indicating that default hyperparameters will be used.\n",
    "\n",
    "    ```python\n",
    "    return params\n",
    "    ```\n",
    "    - **Purpose:** Returns the final hyperparameters to be used in model training.\n",
    "    - **Details:**\n",
    "      - The returned `params` dictionary contains either the default hyperparameters or the ones updated with NNI's suggestions.\n",
    "\n",
    "    **Usage Examples:**\n",
    "\n",
    "    - **With NNI:**\n",
    "      - When running hyperparameter tuning experiments with NNI, this function will integrate seamlessly, allowing NNI to provide new hyperparameters for each trial.\n",
    "      - Example in training script:\n",
    "        ```python\n",
    "        params = get_nni_params()\n",
    "        model = MyModel(**params)\n",
    "        ```\n",
    "      - NNI's tuner will modify the hyperparameters during each trial to find the optimal configuration.\n",
    "\n",
    "    - **Without NNI:**\n",
    "      - If NNI is not being used, the function will return the default hyperparameters.\n",
    "      - The rest of your code can proceed without any modifications.\n",
    "      - Example in training script:\n",
    "        ```python\n",
    "        params = get_nni_params()\n",
    "        model = MyModel(**params)\n",
    "        ```\n",
    "      - The default hyperparameters defined in the function will be used.\n",
    "\n",
    "    **Notes on NNI Integration:**\n",
    "\n",
    "    - **NNI (Neural Network Intelligence):**\n",
    "      - An open-source toolkit for hyperparameter optimization and neural architecture search.\n",
    "      - Provides functionalities to automatically search for the best hyperparameters for machine learning models.\n",
    "      - Official documentation: [NNI Documentation](https://nni.readthedocs.io/en/latest/)\n",
    "\n",
    "    - **Function `nni.get_next_parameter()`:**\n",
    "      - Used to retrieve the hyperparameters for the next trial from NNI's tuner.\n",
    "      - Returns a dictionary with hyperparameters that can overwrite the defaults.\n",
    "\n",
    "    - **Handling Absence of NNI:**\n",
    "      - The `try-except` block ensures that the function doesn't fail when NNI is not being used.\n",
    "      - This makes the function flexible and usable in both development and production environments.\n",
    "\n",
    "    **Best Practices:**\n",
    "\n",
    "    - **Exception Handling:**\n",
    "      - Always include exception handling when calling external libraries that may not be available in all environments.\n",
    "      - This prevents the code from crashing and provides a fallback mechanism.\n",
    "\n",
    "    - **Documentation:**\n",
    "      - Provide clear docstrings explaining the purpose and functionality of the function.\n",
    "      - Include explanations for each line or block of code for better readability and maintainability.\n",
    "\n",
    "    - **Type Annotations:**\n",
    "      - Use type hints (e.g., `-> Dict[str, Any]`) to specify the expected return type of the function.\n",
    "      - This helps with static analysis and improves code clarity.\n",
    "\n",
    "    - **Modularity:**\n",
    "      - Encapsulate hyperparameter retrieval in a separate function to keep the code organized.\n",
    "      - Makes it easier to manage hyperparameters and integrate with tools like NNI.\n",
    "\n",
    "    **Dependencies:**\n",
    "\n",
    "    - **Required Libraries:**\n",
    "      - `nni`: Ensure that the NNI library is installed (`pip install nni`) if you intend to use it.\n",
    "      - `typing`: Used for type annotations.\n",
    "\n",
    "    - **Import Statements:**\n",
    "      ```python\n",
    "      import nni\n",
    "      from typing import Dict, Any\n",
    "      ```\n",
    "\n",
    "    **Possible Modifications:**\n",
    "\n",
    "    - **Adding More Hyperparameters:**\n",
    "      - You can expand the `params` dictionary with additional hyperparameters as needed.\n",
    "        ```python\n",
    "        params = {\n",
    "            'learning_rate': 0.01,\n",
    "            'optimizer_betas': (0.9, 0.99),\n",
    "            'fast_sigmoid_slope': 10,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 10,\n",
    "        }\n",
    "        ```\n",
    "\n",
    "    - **Customizing Exception Handling:**\n",
    "      - Instead of printing a message, you could log the exception or handle it differently depending on your application's requirements.\n",
    "\n",
    "    **Conclusion:**\n",
    "\n",
    "    - The `get_nni_params` function is a robust and flexible way to manage hyperparameters for your machine learning models.\n",
    "    - By integrating with NNI, it allows for automated hyperparameter tuning.\n",
    "    - The inclusion of exception handling ensures that the function remains functional even when NNI is not being used, making it suitable for various stages of development and deployment.\n",
    "\n",
    "    **References:**\n",
    "\n",
    "    - [NNI Documentation](https://nni.readthedocs.io/en/latest/)\n",
    "    - [Python Type Hints (PEP 484)](https://www.python.org/dev/peps/pep-0484/)\n",
    "    - [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)\n",
    "    - [PEP 257 -- Docstring Conventions](https://www.python.org/dev/peps/pep-0257/)\n",
    "\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer_betas': (0.9, 0.99),\n",
    "        'fast_sigmoid_slope': 10,\n",
    "    }\n",
    "    try:\n",
    "        # Update with parameters from NNI\n",
    "        tuner_params = nni.get_next_parameter()\n",
    "        params.update(tuner_params)\n",
    "    except Exception as e:\n",
    "        print(\"NNI is not being used or failed to retrieve parameters. Using default hyperparameters.\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiking Neural Network Model\n",
    "class SNNQUT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output, hidden_reset_mechanism, output_reset_mechanism, hidden_threshold, output_threshold, fast_sigmoid_slope,):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta_hidden_1, reset_mechanism=hidden_reset_mechanism, threshold=hidden_threshold, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta_hidden_2, reset_mechanism=hidden_reset_mechanism,  threshold=hidden_threshold, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif3 = snn.Leaky(beta=beta_hidden_3, reset_mechanism=hidden_reset_mechanism,  threshold=hidden_threshold, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.lif4 = snn.Leaky(beta=beta_output, reset_mechanism=output_reset_mechanism, threshold=output_threshold)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        #self.enforce_weight_constraints() # enforcing weight constraints to have some % of negative weights\n",
    "\n",
    "    def enforce_weight_constraints(self):\n",
    "        # Collect all weight tensors and their shapes\n",
    "        params = []\n",
    "        shapes = []\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                params.append(param)\n",
    "                shapes.append(param.shape)\n",
    "        # Flatten all parameters and concatenate\n",
    "        flat_params = [p.data.view(-1) for p in params]\n",
    "        all_weights = torch.cat(flat_params)\n",
    "        total_weights = all_weights.numel()\n",
    "        num_neg_weights = int(0.5 * total_weights)  # number of negative weights\n",
    "\n",
    "        # Ensure the random indices are on the same device\n",
    "        device = all_weights.device\n",
    "        # Set all weights to absolute values (positive)\n",
    "        all_weights = all_weights.abs()\n",
    "        # Randomly select num_neg_weights indices to be negative\n",
    "        permuted_indices = torch.randperm(total_weights, device=device)\n",
    "        neg_indices = permuted_indices[:num_neg_weights]\n",
    "        # Set selected weights to negative\n",
    "        all_weights[neg_indices] *= -1\n",
    "\n",
    "        # Now, split all_weights back to the parameter shapes\n",
    "        pointer = 0\n",
    "        for i, param in enumerate(params):\n",
    "            numel = param.numel()\n",
    "            param_data = all_weights[pointer:pointer+numel].view(shapes[i])\n",
    "            param.data.copy_(param_data)\n",
    "            pointer += numel\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)  # Convert input to float32\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "\n",
    "        # Initialization of membrane potentials\n",
    "        mem1 = torch.zeros(batch_size, self.fc1.out_features, device=x.device)\n",
    "        mem2 = torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "        mem3 = torch.zeros(batch_size, self.fc3.out_features, device=x.device)\n",
    "        mem4 = torch.zeros(batch_size, self.fc4.out_features, device=x.device)\n",
    "\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        mem4_rec = []\n",
    "\n",
    "        for step in range(time_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            # Record at every time step\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem4_rec.append(mem4)\n",
    "\n",
    "        # Stack along the time axis (first dimension)\n",
    "        return torch.stack(spk1_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(spk3_rec, dim=0), torch.stack(mem4_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning Module\n",
    "class Lightning_SNNQUT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        beta_hidden_1,\n",
    "        beta_hidden_2,\n",
    "        beta_hidden_3,\n",
    "        beta_output,\n",
    "        hidden_reset_mechanism,\n",
    "        output_reset_mechanism,\n",
    "        hidden_threshold,\n",
    "        output_threshold,\n",
    "        learning_rate,\n",
    "        scheduler_step_size,\n",
    "        scheduler_gamma,\n",
    "        optimizer_betas,\n",
    "        fast_sigmoid_slope,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\n",
    "            'input_size',\n",
    "            'hidden_size',\n",
    "            'output_size',\n",
    "            'hidden_reset_mechanism',\n",
    "            'output_reset_mechanism',\n",
    "            'hidden_threshold',\n",
    "            'output_threshold',\n",
    "            'learning_rate',\n",
    "            'scheduler_step_size',\n",
    "            'scheduler_gamma',\n",
    "            'optimizer_betas',\n",
    "            'fast_sigmoid_slope',\n",
    "        )\n",
    "        # Assign tensors directly since using self.save_hyperparameters() gives error because beta tensors are not JSON serializable:\n",
    "        # - they are torch tensors, can't convert to JSON\n",
    "        self.beta_hidden_1 = beta_hidden_1\n",
    "        self.beta_hidden_2 = beta_hidden_2\n",
    "        self.beta_hidden_3 = beta_hidden_3\n",
    "        self.beta_output = beta_output\n",
    "\n",
    "        # Initialize confusion matrices for train, val, and test\n",
    "        self.train_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "        self.val_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "        self.test_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize the SNN model\n",
    "        self.model = SNNQUT(\n",
    "            input_size=self.hparams.input_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            beta_hidden_1=self.beta_hidden_1,\n",
    "            beta_hidden_2=self.beta_hidden_2,\n",
    "            beta_hidden_3=self.beta_hidden_3,\n",
    "            beta_output=self.beta_output,\n",
    "            hidden_reset_mechanism=self.hparams.hidden_reset_mechanism,\n",
    "            output_reset_mechanism=self.hparams.output_reset_mechanism,\n",
    "            output_threshold=self.hparams.output_threshold,\n",
    "            hidden_threshold=self.hparams.hidden_threshold,\n",
    "            fast_sigmoid_slope=self.hparams.fast_sigmoid_slope,\n",
    "        )\n",
    "\n",
    "        # Initialize the loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log_weight_statistics('train')\n",
    "        if self.train_confmat._update_called:  # Check if `update` has been called\n",
    "            confmat = self.train_confmat.compute()\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(confmat.cpu().numpy(), annot=True, fmt='d', cmap='Blues')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.title('Train Confusion Matrix')\n",
    "            self.logger.experiment.add_figure('train_confusion_matrix', fig, self.current_epoch)\n",
    "            plt.close(fig)\n",
    "            self.train_confmat.reset()\n",
    "        else:\n",
    "            print(\"Warning: No data to compute train confusion matrix\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_weight_statistics('val')\n",
    "        if self.val_confmat._update_called:  # Check if `update` has been called\n",
    "            confmat = self.val_confmat.compute()\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(confmat.cpu().numpy(), annot=True, fmt='d', cmap='Blues')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.title('Validation Confusion Matrix')\n",
    "            self.logger.experiment.add_figure('val_confusion_matrix', fig, self.current_epoch)\n",
    "            plt.close(fig)\n",
    "            self.val_confmat.reset()\n",
    "        else:\n",
    "            print(\"Warning: No data to compute validation confusion matrix\")\n",
    "\n",
    "\n",
    "    def log_weight_statistics(self, mode):\n",
    "        total_neg_weights = 0\n",
    "        total_weights = 0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weight_mean = param.data.mean()\n",
    "                weight_std = param.data.std()\n",
    "                num_neg = (param.data < 0).sum().item()\n",
    "                num_total = param.data.numel()\n",
    "                total_neg_weights += num_neg\n",
    "                total_weights += num_total\n",
    "                self.log(f'{mode}_weight_mean_{name}', weight_mean, on_epoch=True, prog_bar=False)\n",
    "                self.log(f'{mode}_weight_std_{name}', weight_std, on_epoch=True, prog_bar=False)\n",
    "                self.log(f'{mode}_num_neg_{name}', num_neg, on_epoch=True, prog_bar=False)\n",
    "        percent_neg_weights = (total_neg_weights / total_weights) * 100\n",
    "        self.log(f'{mode}_percent_neg_weights', percent_neg_weights, on_epoch=True, prog_bar=False)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # Logging membrane potentials\n",
    "        self.logger.experiment.add_scalar('spk1_sum', spk1_rec.sum(), self.global_step)\n",
    "        self.logger.experiment.add_scalar('spk2_sum', spk2_rec.sum(), self.global_step)\n",
    "        self.logger.experiment.add_scalar('spk3_sum', spk3_rec.sum(), self.global_step)\n",
    "\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded * Vmem_shift_for_MSELoss) + Vmem_shift_for_MSELoss)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Update confusion matrix\n",
    "        self.train_confmat.update(predicted, targets)\n",
    "\n",
    "        # Log training loss and accuracy\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_accuracy', accuracy * 100, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        # Log the gradient norm (average of the absolute value of the gradients) and gradient mean, for each parameter\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.abs().mean()\n",
    "                grad_mean = param.grad.mean()\n",
    "                self.log(f'grad_norm_{name}', grad_norm, on_step=True, on_epoch=True, prog_bar=False)\n",
    "                self.log(f'grad_mean/{name}', grad_mean, on_step=True, on_epoch=True, prog_bar=False)\n",
    "            else:\n",
    "                print(f'No gradient for parameter: {name}')\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded * Vmem_shift_for_MSELoss) + Vmem_shift_for_MSELoss)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Update confusion matrix\n",
    "        self.val_confmat.update(predicted, targets)\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded * Vmem_shift_for_MSELoss) + Vmem_shift_for_MSELoss)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Update confusion matrix\n",
    "        self.test_confmat.update(predicted, targets)\n",
    "\n",
    "        # Log test loss and accuracy\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy * 100, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "\n",
    "    # def on_before_zero_grad(self, optimizer):\n",
    "    #         for param in self.model.parameters():\n",
    "    #             if param.requires_grad:\n",
    "    #                 param.data.clamp_(min=0.001)\n",
    "\n",
    "# enforcing weight constraints to have some % of negative weights, use EITHER clamp or enforce_weight_constraints\n",
    "    # def on_before_zero_grad(self, optimizer):\n",
    "    #         self.model.enforce_weight_constraints() \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = optim.Adamax(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=self.hparams.optimizer_betas,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hparams.scheduler_step_size,\n",
    "            gamma=self.hparams.scheduler_gamma,\n",
    "        )\n",
    "        \n",
    "        return [optimizer]  , [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Module\n",
    "class QUTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = QUTDataset(self.data_dir)\n",
    "        train_size = int(0.65 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Tau and Beta Values\n",
    "def generate_tau_beta_values(hidden_size, output_size):\n",
    "\n",
    "    def create_power_vector(n, size):\n",
    "        # Generate the powers of 2 up to 2^n\n",
    "        powers = [2 ** i for i in range(1, n + 1)]\n",
    "        # Calculate how many times each power should be repeated\n",
    "        repeat_count = size // n\n",
    "        # Create the final vector by repeating each power equally\n",
    "        power_vector = np.repeat(powers, repeat_count)\n",
    "        return power_vector\n",
    "\n",
    "    # Generate Tau Values\n",
    "    size = hidden_size\n",
    "    tau_hidden_1 = create_power_vector(n=2, size=size)\n",
    "    tau_hidden_2 = create_power_vector(n=4, size=size)\n",
    "    tau_hidden_3 = create_power_vector(n=8, size=size)\n",
    "\n",
    "    # Generate Beta Values from Tau\n",
    "    delta_t = 1  # 1ms time step\n",
    "\n",
    "    beta_hidden_1 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_1, dtype=torch.float32))\n",
    "    beta_hidden_2 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_2, dtype=torch.float32))\n",
    "    beta_hidden_3 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_3, dtype=torch.float32))\n",
    "\n",
    "    tau_output = np.repeat(10, output_size)\n",
    "    beta_output = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_output, dtype=torch.float32))\n",
    "\n",
    "    # Return all beta values\n",
    "    return beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function, so it's easy to convert to script\n",
    "def main():\n",
    "    # Get hyperparameters from NNI\n",
    "    params = get_nni_params()\n",
    "    beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output = generate_tau_beta_values(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Initialize the data module\n",
    "    # data_dir is either TEST or 4_one_second_samples\n",
    "    data_dir = 'data/TEST'\n",
    "    data_module = QUTDataModule(\n",
    "        data_dir, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # Initialize the Lightning model with hyperparameters from NNI\n",
    "    model = Lightning_SNNQUT(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        beta_hidden_1=beta_hidden_1,\n",
    "        beta_hidden_2=beta_hidden_2,\n",
    "        beta_hidden_3=beta_hidden_3,\n",
    "        beta_output=beta_output,\n",
    "        hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "        output_reset_mechanism=output_reset_mechanism,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        optimizer_betas=params['optimizer_betas'],\n",
    "        scheduler_step_size=scheduler_step_size,\n",
    "        scheduler_gamma=scheduler_gamma,\n",
    "        output_threshold=output_threshold,\n",
    "        hidden_threshold=hidden_threshold,\n",
    "        fast_sigmoid_slope=params['fast_sigmoid_slope'],\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir='logs',name='Mikel_LIF')\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        log_every_n_steps=10,\n",
    "        logger=logger,\n",
    "        #accelerator='gpu' if torch.cuda.is_available() else 'cpu', devices='auto',\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # Validate the model\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "    val_accuracy = trainer.callback_metrics['val_accuracy'].item()\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(model, datamodule=data_module)\n",
    "    test_accuracy = trainer.callback_metrics['test_accuracy'].item()\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Report the result to NNI\n",
    "    nni.report_final_result(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs/Mikel_LIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To add:\n",
    "\n",
    "- checkpoints and extra training\n",
    "- log spikes, Vmem, ...\n",
    "- learning rate finder (optional)\n",
    "- thorough documentation \n",
    "- loss dependent on num spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroVecio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
