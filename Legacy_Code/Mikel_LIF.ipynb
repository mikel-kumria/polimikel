{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nni\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from snntorch import surrogate\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and Constants\n",
    "input_size = 16\n",
    "hidden_size = 24\n",
    "output_size = 4\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "scheduler_step_size = 3\n",
    "scheduler_gamma = 0.5 # halving the LR every step_size epochs\n",
    "\n",
    "# Adjusted after training for test\n",
    "output_threshold = 1e7\n",
    "hidden_threshold = 1\n",
    "\n",
    "num_workers = max(1, os.cpu_count() - 1)\n",
    "hidden_reset_mechanism = 'subtract'\n",
    "output_reset_mechanism = 'none'\n",
    "\n",
    "Vmem_shift_for_MSELoss = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"CAR\", \"STREET\", \"HOME\", \"CAFE\"]\n",
    "\n",
    "def get_label_from_folder(folder_name):\n",
    "    # folder_name format: \"CAR-something\", extract \"CAR\"\n",
    "    base = os.path.basename(folder_name)\n",
    "    label = base.split('-')[0]\n",
    "    return label\n",
    "\n",
    "def one_hot_encode(label):\n",
    "    # from label in CLASSES -> CAR will be [1, 0, 0, 0], STREET will be [0, 1, 0, 0], following the CLASSES order\n",
    "    idx = CLASSES.index(label)\n",
    "    vec = np.zeros(len(CLASSES), dtype=np.float32)\n",
    "    vec[idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "def get_nni_params():\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer_betas': (0.9, 0.99),\n",
    "        'fast_sigmoid_slope': 10,\n",
    "    }\n",
    "    tuner_params = nni.get_next_parameter()\n",
    "    params.update(tuner_params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantize5bit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.levels = 32  # 2^5 = 32 levels = 5 bits\n",
    "        self.w_min = 0.001\n",
    "        self.w_max = 1\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_clamped = torch.clamp(input, self.w_min, self.w_max)\n",
    "        scale = (self.w_max - self.w_min) / (self.levels - 1)\n",
    "        quant_indices = torch.round((input_clamped - self.w_min) / scale)\n",
    "        quant_w = quant_indices * scale + self.w_min\n",
    "        return quant_w\n",
    "\n",
    "class QuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "        self.fake_quant = FakeQuantize5bit()\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant_weight = self.fake_quant(self.weight)\n",
    "        return F.linear(input, quant_weight, self.bias)\n",
    "\n",
    "def finalize_quantization(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'fake_quant') and hasattr(module, 'weight'):\n",
    "                quantized_weight = module.fake_quant(module.weight)\n",
    "                module.weight.data.copy_(quantized_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNQUT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output, \n",
    "                 hidden_reset_mechanism, output_reset_mechanism, \n",
    "                 hidden_threshold, output_threshold, fast_sigmoid_slope):\n",
    "        super().__init__()\n",
    "        self.fc1 = QuantLinear(input_size, hidden_size, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta_hidden_1, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc2 = QuantLinear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta_hidden_2, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc3 = QuantLinear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif3 = snn.Leaky(beta=beta_hidden_3, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc4 = QuantLinear(hidden_size, output_size, bias=False)\n",
    "        self.lif4 = snn.Leaky(beta=beta_output, reset_mechanism=output_reset_mechanism,\n",
    "                              threshold=output_threshold)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time_steps, 16)\n",
    "        x = x.float()\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        mem1 = torch.zeros(batch_size, self.fc1.out_features, device=device)\n",
    "        mem2 = torch.zeros(batch_size, self.fc2.out_features, device=device)\n",
    "        mem3 = torch.zeros(batch_size, self.fc3.out_features, device=device)\n",
    "        mem4 = torch.zeros(batch_size, self.fc4.out_features, device=device)\n",
    "\n",
    "        spk1_rec, spk2_rec, spk3_rec, mem4_rec = [], [], [], []\n",
    "\n",
    "        for step in range(time_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            spk4_rec.append(spk4)\n",
    "            mem4_rec.append(mem4)\n",
    "\n",
    "        return (torch.stack(spk1_rec, dim=0), torch.stack(spk2_rec, dim=0),\n",
    "                torch.stack(spk3_rec, dim=0), torch.stack(spk4_rec, dim=0),\n",
    "                torch.stack(mem4_rec, dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lightning_SNNQUT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        beta_hidden_1,\n",
    "        beta_hidden_2,\n",
    "        beta_hidden_3,\n",
    "        beta_output,\n",
    "        hidden_reset_mechanism,\n",
    "        output_reset_mechanism,\n",
    "        hidden_threshold,\n",
    "        output_threshold,\n",
    "        learning_rate,\n",
    "        scheduler_step_size,\n",
    "        scheduler_gamma,\n",
    "        optimizer_betas,\n",
    "        fast_sigmoid_slope,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\n",
    "            'input_size',\n",
    "            'hidden_size',\n",
    "            'output_size',\n",
    "            'hidden_reset_mechanism',\n",
    "            'output_reset_mechanism',\n",
    "            'hidden_threshold',\n",
    "            'output_threshold',\n",
    "            'learning_rate',\n",
    "            'scheduler_step_size',\n",
    "            'scheduler_gamma',\n",
    "            'optimizer_betas',\n",
    "            'fast_sigmoid_slope',\n",
    "        )\n",
    "        self.beta_hidden_1 = beta_hidden_1\n",
    "        self.beta_hidden_2 = beta_hidden_2\n",
    "        self.beta_hidden_3 = beta_hidden_3\n",
    "        self.beta_output = beta_output\n",
    "\n",
    "        self.train_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "        self.val_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "        self.test_confmat = ConfusionMatrix(task='multiclass',num_classes=self.hparams.output_size)\n",
    "\n",
    "        self.model = SNNQUT(\n",
    "            input_size=self.hparams.input_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            beta_hidden_1=self.beta_hidden_1,\n",
    "            beta_hidden_2=self.beta_hidden_2,\n",
    "            beta_hidden_3=self.beta_hidden_3,\n",
    "            beta_output=self.beta_output,\n",
    "            hidden_reset_mechanism=self.hparams.hidden_reset_mechanism,\n",
    "            output_reset_mechanism=self.hparams.output_reset_mechanism,\n",
    "            output_threshold=self.hparams.output_threshold,\n",
    "            hidden_threshold=self.hparams.hidden_threshold,\n",
    "            fast_sigmoid_slope=self.hparams.fast_sigmoid_slope,\n",
    "        )\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "        # Remove unused commented-out code, ensure comments accurately reflect behavior\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss_and_metrics(self, spk4_rec, mem4_rec, labels, mode='Vmem'):\n",
    "        \"\"\"\n",
    "        Compute loss and classification metrics.\n",
    "        mode='Vmem' uses mem4_rec for classification (e.g., training/validation).\n",
    "        mode='spike' uses spk4_rec for classification (e.g., testing if desired).\n",
    "        \"\"\"\n",
    "        # Compute loss using mem4_rec\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded + Vmem_shift_for_MSELoss))\n",
    "\n",
    "        # Compute predictions\n",
    "        if mode == 'Vmem':\n",
    "            # Use membrane potentials summed over time for prediction\n",
    "            final_out = mem4_rec.sum(0)  # (batch, output_size)\n",
    "        else:\n",
    "            # Use spikes summed over time for prediction\n",
    "            final_out = spk4_rec.sum(0)  # (batch, output_size)\n",
    "\n",
    "        _, predicted = final_out.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "        return loss, accuracy, predicted, targets\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, spk4_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # Use 'mem' mode during training for consistency with training approach\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4_rec, mem4_rec, labels, mode='Vmem')\n",
    "\n",
    "        self.train_confmat.update(predicted, targets)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_accuracy', accuracy * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, spk4_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # For validation, also use 'mem' mode to stay consistent\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4_rec, mem4_rec, labels, mode='Vmem')\n",
    "\n",
    "        self.val_confmat.update(predicted, targets)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy * 100, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1_rec, spk2_rec, spk3_rec, spk4_rec, mem4_rec = self(inputs)\n",
    "\n",
    "        # For testing, use 'spike' mode if you want to classify based on spikes\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4_rec, mem4_rec, labels, mode='spike')\n",
    "\n",
    "        self.test_confmat.update(predicted, targets)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adamax(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=self.hparams.optimizer_betas,\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hparams.scheduler_step_size,\n",
    "            gamma=self.hparams.scheduler_gamma,\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSNNTrainValDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_list=CLASSES):\n",
    "        self.samples = []  # list of (data_tensor, label_tensor)\n",
    "        self.class_list = class_list\n",
    "\n",
    "        # Load all samples from each class directory\n",
    "        # Assuming structure: root_dir/<LABEL>-something/*.csv\n",
    "        for folder in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            label = get_label_from_folder(folder_path)\n",
    "            if label not in self.class_list:\n",
    "                continue\n",
    "            label_vec = one_hot_encode(label)\n",
    "            # load all csv files\n",
    "            csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "            for csv_file in csv_files:\n",
    "                data_array = self.load_csv(csv_file, time_steps=100, input_dim=16)\n",
    "                self.samples.append((data_array, label_vec))\n",
    "\n",
    "    def load_csv(self, csv_path, time_steps=1000, input_dim=16):\n",
    "        # load a 1000x16 csv\n",
    "        data = np.loadtxt(csv_path, delimiter=',', dtype=np.float32)\n",
    "        # shape should be (1000,16)\n",
    "        # if data.shape != (time_steps, input_dim):\n",
    "        #     raise ValueError(f\"CSV {csv_path} shape mismatch: {data.shape}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.samples[idx]\n",
    "        # return as tensors\n",
    "        return torch.tensor(data), torch.tensor(label)\n",
    "\n",
    "\n",
    "def stratified_split(dataset, train_ratio=0.8):\n",
    "    # Simple stratified split:\n",
    "    # Count how many samples per class\n",
    "    labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, l = dataset[i]\n",
    "        c = torch.argmax(l).item()\n",
    "        labels.append(c)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "\n",
    "    for c in np.unique(labels):\n",
    "        class_indices = np.where(labels == c)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "        split_point = int(len(class_indices)*train_ratio)\n",
    "        train_part = class_indices[:split_point]\n",
    "        val_part = class_indices[split_point:]\n",
    "        train_indices.extend(train_part)\n",
    "        val_indices.extend(val_part)\n",
    "\n",
    "    # shuffle overall?\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    # create subsets\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    return train_subset, val_subset\n",
    "\n",
    "\n",
    "class CustomSNNTestDataset(Dataset):\n",
    "    def __init__(self, test_dir, class_list=CLASSES):\n",
    "        self.samples = []\n",
    "        # Each class folder has one large csv (60000 x 16)\n",
    "        for folder in os.listdir(test_dir):\n",
    "            folder_path = os.path.join(test_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            label = get_label_from_folder(folder_path)\n",
    "            if label not in class_list:\n",
    "                continue\n",
    "            label_vec = one_hot_encode(label)\n",
    "            # assume one test.csv per class folder\n",
    "            csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "            if len(csv_files) == 0:\n",
    "                raise ValueError(f\"No test csv found in {folder_path}\")\n",
    "            # just take the first csv file\n",
    "            csv_file = csv_files[0]\n",
    "            data_array = self.load_csv(csv_file, time_steps=6000, input_dim=16)\n",
    "            self.samples.append((data_array, label_vec))\n",
    "\n",
    "    def load_csv(self, csv_path, time_steps=60000, input_dim=16):\n",
    "        data = np.loadtxt(csv_path, delimiter=',', dtype=np.float32)\n",
    "        if data.shape != (time_steps, input_dim):\n",
    "            raise ValueError(f\"Test CSV {csv_path} shape mismatch: {data.shape}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.samples[idx]\n",
    "        return torch.tensor(data), torch.tensor(label)\n",
    "\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_val_dir, test_dir, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_val_dir = train_val_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            full_dataset = CustomSNNTrainValDataset(self.train_val_dir)\n",
    "            self.train_dataset, self.val_dataset = stratified_split(full_dataset, train_ratio=0.8)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = CustomSNNTestDataset(self.test_dir)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False,\n",
    "                          num_workers=self.num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tau_beta_values(hidden_size, output_size):\n",
    "    def create_power_vector(n, size):\n",
    "        powers = [2 ** i for i in range(1, n + 1)]\n",
    "        repeat_count = size // n\n",
    "        power_vector = np.repeat(powers, repeat_count)\n",
    "        return power_vector\n",
    "\n",
    "    size = hidden_size\n",
    "    tau_hidden_1 = create_power_vector(n=2, size=size)\n",
    "    tau_hidden_2 = create_power_vector(n=4, size=size)\n",
    "    tau_hidden_3 = create_power_vector(n=8, size=size)\n",
    "\n",
    "    delta_t = 1\n",
    "    beta_hidden_1 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_1, dtype=torch.float32))\n",
    "    beta_hidden_2 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_2, dtype=torch.float32))\n",
    "    beta_hidden_3 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_3, dtype=torch.float32))\n",
    "\n",
    "    tau_output = np.repeat(10, output_size)\n",
    "    beta_output = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_output, dtype=torch.float32))\n",
    "\n",
    "    return beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    params = get_nni_params()\n",
    "    beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output = generate_tau_beta_values(hidden_size, output_size)\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Directories\n",
    "    train_val_dir = 'data/PROCESSED_YES_COCHLEA/CUT/TRAIN_VAL'  # directory containing CAR-..., STREET-..., HOME-..., CAFE-...\n",
    "    test_dir = 'data/PROCESSED_YES_COCHLEA/CUT/TEST'            # directory containing test sets for each class\n",
    "\n",
    "    data_module = CustomDataModule(\n",
    "        train_val_dir=train_val_dir,\n",
    "        test_dir=test_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    data_module.setup('fit')\n",
    "\n",
    "    model = Lightning_SNNQUT(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        beta_hidden_1=beta_hidden_1,\n",
    "        beta_hidden_2=beta_hidden_2,\n",
    "        beta_hidden_3=beta_hidden_3,\n",
    "        beta_output=beta_output,\n",
    "        hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "        output_reset_mechanism=output_reset_mechanism,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        optimizer_betas=params['optimizer_betas'],\n",
    "        scheduler_step_size=scheduler_step_size,\n",
    "        scheduler_gamma=scheduler_gamma,\n",
    "        output_threshold=output_threshold,\n",
    "        hidden_threshold=hidden_threshold,\n",
    "        fast_sigmoid_slope=params['fast_sigmoid_slope'],\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir='logs', name='Mikel_LIF_quant_5bit')\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        log_every_n_steps=10,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # After training, finalize quantization\n",
    "    finalize_quantization(model.model)\n",
    "\n",
    "    # Adjust lif4 threshold and reset mechanism for testing\n",
    "    model.model.lif4.threshold = 1.0\n",
    "    model.model.lif4.reset_mechanism = 'subtract'\n",
    "\n",
    "    # Validate\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "    val_accuracy = trainer.callback_metrics.get('val_accuracy', torch.tensor(0)).item()\n",
    "\n",
    "    # Test\n",
    "    # Setup test\n",
    "    data_module.setup('test')\n",
    "    trainer.test(model, datamodule=data_module)\n",
    "    test_accuracy = trainer.callback_metrics.get('test_accuracy', torch.tensor(0)).item()\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    nni.report_final_result(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To add:\n",
    "\n",
    "- checkpoints and extra training\n",
    "- log spikes, Vmem, ...\n",
    "- learning rate finder (optional)\n",
    "- thorough documentation \n",
    "- loss dependent on num spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroVecio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
