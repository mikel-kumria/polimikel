{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nni_params():\n",
    "    # Default hyperparameters\n",
    "    params = {\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer_betas': (0.9, 0.999),\n",
    "        'fast_sigmoid_slope': 20.0,\n",
    "    }\n",
    "    # Update with parameters from NNI\n",
    "    tuner_params = nni.get_next_parameter()\n",
    "    params.update(tuner_params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import torch.optim as optim\n",
    "import torch.backends.mps\n",
    "import random\n",
    "import numpy as np\n",
    "import nni\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from QUT_DataLoader import QUTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 16\n",
    "hidden_size = 24\n",
    "output_size = 4\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "optimizer_betas = (0.9, 0.999)\n",
    "scheduler_step_size = 20\n",
    "scheduler_gamma = 0.2\n",
    "learning_rate = 0.05\n",
    "num_workers = os.cpu_count()\n",
    "cuba_tau = 2 # 2ms of tau_Vmem of the CUBA neuron\n",
    "hidden_reset_mechanism = 'subtract'\n",
    "output_reset_mechanism = 'none'\n",
    "output_threshold = 10000\n",
    "hid_threshold = 0.001\n",
    "# logger = TensorBoardLogger('tb_logs', name='snn_QUT_TensorBoardLogger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNQUT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        beta_hidden_1,\n",
    "        beta_hidden_2,\n",
    "        beta_hidden_3,\n",
    "        beta_output,\n",
    "        cuba_beta,\n",
    "        hidden_reset_mechanism,\n",
    "        output_reset_mechanism,\n",
    "        output_threshold,\n",
    "        hid_threshold,\n",
    "        fast_sigmoid_slope,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        # here for CUBA I am using alpha = the betas of the previous LIF neurons, and beta = cuba_beta, which is extracted from cuba_tau\n",
    "        self.lif1 = snn.Synaptic(alpha=beta_hidden_1, beta=cuba_beta, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope), reset_mechanism=hidden_reset_mechanism, threshold=hid_threshold, learn_threshold=False)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif2 = snn.Synaptic(alpha=beta_hidden_2, beta=cuba_beta, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope), reset_mechanism=hidden_reset_mechanism, threshold=hid_threshold, learn_threshold=False)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif3 = snn.Synaptic(alpha=beta_hidden_3, beta=cuba_beta, spike_grad=snn.surrogate.fast_sigmoid(slope=fast_sigmoid_slope), reset_mechanism=hidden_reset_mechanism, threshold=hid_threshold, learn_threshold=False)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.lif4 = snn.Leaky(\n",
    "            beta=beta_output,\n",
    "            reset_mechanism=output_reset_mechanism,\n",
    "            threshold=output_threshold,\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # Initialization with normal distribution, let's try xavier for better convergence\n",
    "    # def _initialize_weights(self):\n",
    "    #     nn.init.normal_(self.fc1.weight, mean=1.0, std=0.1)\n",
    "    #     nn.init.normal_(self.fc2.weight, mean=0.3, std=0.1)\n",
    "    #     nn.init.normal_(self.fc3.weight, mean=0.2, std=0.1)\n",
    "    #     nn.init.normal_(self.fc4.weight, mean=0.1, std=0.1)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)  # Convert input to float32\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "\n",
    "        # Initialization of membrane potentials\n",
    "        mem1 = torch.zeros(batch_size, self.fc1.out_features, device=x.device)\n",
    "        mem2 = torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "        mem3 = torch.zeros(batch_size, self.fc3.out_features, device=x.device)\n",
    "        mem4 = torch.zeros(batch_size, self.fc4.out_features, device=x.device)\n",
    "\n",
    "        syn1 = torch.zeros(batch_size, self.fc1.out_features, device=x.device)\n",
    "        syn2 = torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "        syn3 = torch.zeros(batch_size, self.fc3.out_features, device=x.device)\n",
    "\n",
    "        mem4_rec = []\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(time_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            # Record at every time step\n",
    "            mem4_rec.append(mem4)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "        return torch.stack(mem4_rec, dim=0), torch.stack(spk3_rec, dim=0) # so they will be stacked along the time axis (1000 steps) on the first dimension (dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lightning_SNNQUT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        beta_hidden_1,\n",
    "        beta_hidden_2,\n",
    "        beta_hidden_3,\n",
    "        beta_output,\n",
    "        hidden_reset_mechanism,\n",
    "        output_reset_mechanism,\n",
    "        hid_threshold,\n",
    "        output_threshold,\n",
    "        cuba_beta,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer_betas=optimizer_betas,\n",
    "        scheduler_step_size=scheduler_step_size,\n",
    "        scheduler_gamma=scheduler_gamma,\n",
    "        fast_sigmoid_slope=20.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate', 'optimizer_betas', 'fast_sigmoid_slope')\n",
    "\n",
    "        # Initialize the SNN model\n",
    "        self.model = SNNQUT(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            beta_hidden_1=beta_hidden_1,\n",
    "            beta_hidden_2=beta_hidden_2,\n",
    "            beta_hidden_3=beta_hidden_3,\n",
    "            beta_output=beta_output,\n",
    "            cuba_beta=cuba_beta,\n",
    "            hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "            output_reset_mechanism=output_reset_mechanism,\n",
    "            output_threshold=output_threshold,\n",
    "            hid_threshold = hid_threshold,\n",
    "            fast_sigmoid_slope=fast_sigmoid_slope,\n",
    "        )\n",
    "\n",
    "        # Initialize the loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        mem4_rec, spk3_rec = self(inputs)\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded*5)+5)\n",
    "        # loss = self.loss_function(mem4_rec, labels_expanded)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_accuracy', accuracy*100, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        mem4_rec, spk3_rec = self(inputs)\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded*5)+5)\n",
    "        # loss = self.loss_function(mem4_rec, labels_expanded)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Log validation loss\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        mem4_rec, spk3_rec = self(inputs)\n",
    "\n",
    "        # Expanding labels to match mem4_rec's shape\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded*5)+5)\n",
    "        # loss = self.loss_function(mem4_rec, labels_expanded)\n",
    "\n",
    "        # Use the final membrane potential for prediction\n",
    "        final_mem4 = mem4_rec.sum(0)\n",
    "\n",
    "        # Predicted class is the one with the highest membrane potential\n",
    "        _, predicted = final_mem4.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        # For checking mems\n",
    "        #print(mem4_rec[:,0])\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Log test loss\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy*100, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=self.hparams.optimizer_betas,\n",
    "        )\n",
    "        # scheduler = optim.lr_scheduler.StepLR(\n",
    "        #     optimizer,\n",
    "        #     step_size=self.hparams.scheduler_step_size,\n",
    "        #     gamma=self.hparams.scheduler_gamma,\n",
    "        # )\n",
    "        return [optimizer]#, [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QUTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = QUTDataset(self.data_dir)\n",
    "        train_size = int(0.65 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tau and Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: tau_hidden_1 = [2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Vector: tau_hidden_2 = [ 2  2  2  2  2  2  4  4  4  4  4  4  8  8  8  8  8  8 16 16 16 16 16 16]\n",
      "Vector: tau_hidden_3 = [  2   2   2   4   4   4   8   8   8  16  16  16  32  32  32  64  64  64\n",
      " 128 128 128 256 256 256]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "        GEOMETRIC SERIES TAU\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def create_power_vector(n, size):        \n",
    "    # Generate the powers of 2 up to 2^n\n",
    "    powers = [2**i for i in range(1, n+1)]\n",
    "    \n",
    "    # Calculate how many times each power should be repeated\n",
    "    repeat_count = size // n\n",
    "    \n",
    "    # Create the final vector by repeating each power equally\n",
    "    vector = np.repeat(powers, repeat_count)\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "size = 24\n",
    "n = 2\n",
    "tau_hidden_1 = create_power_vector(n, size)\n",
    "print(f\"Vector: tau_hidden_1 = {tau_hidden_1}\")\n",
    "\n",
    "size = 24\n",
    "n = 4\n",
    "tau_hidden_2 = create_power_vector(n, size)\n",
    "print(f\"Vector: tau_hidden_2 = {tau_hidden_2}\")\n",
    "\n",
    "size = 24\n",
    "n = 8\n",
    "tau_hidden_3 = create_power_vector(n, size)\n",
    "print(f\"Vector: tau_hidden_3 = {tau_hidden_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beta_hidden_1: tensor([0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.6065,\n",
      "        0.6065, 0.6065, 0.6065, 0.7788, 0.7788, 0.7788, 0.7788, 0.7788, 0.7788,\n",
      "        0.7788, 0.7788, 0.7788, 0.7788, 0.7788, 0.7788])\n",
      "\n",
      "beta_hidden_2: tensor([0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.6065, 0.7788, 0.7788, 0.7788,\n",
      "        0.7788, 0.7788, 0.7788, 0.8825, 0.8825, 0.8825, 0.8825, 0.8825, 0.8825,\n",
      "        0.9394, 0.9394, 0.9394, 0.9394, 0.9394, 0.9394])\n",
      "\n",
      "beta_hidden_3: tensor([0.6065, 0.6065, 0.6065, 0.7788, 0.7788, 0.7788, 0.8825, 0.8825, 0.8825,\n",
      "        0.9394, 0.9394, 0.9394, 0.9692, 0.9692, 0.9692, 0.9845, 0.9845, 0.9845,\n",
      "        0.9922, 0.9922, 0.9922, 0.9961, 0.9961, 0.9961])\n",
      "\n",
      "tau_output: [10 10 10 10]\n",
      "\n",
      "beta_output: tensor([0.9048, 0.9048, 0.9048, 0.9048])\n",
      "\n",
      "cuba_beta: 0.6065306663513184\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "        BETA DISTRIBUTION FROM TAU\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "delaT = 1 # 1ms time step\n",
    "\n",
    "\n",
    "beta_hidden_1 = torch.exp(-torch.tensor(delaT, dtype=torch.float32) / torch.tensor(tau_hidden_1, dtype=torch.float32))\n",
    "print(f\"\\nbeta_hidden_1: {beta_hidden_1}\")\n",
    "\n",
    "beta_hidden_2 = torch.exp(-torch.tensor(delaT, dtype=torch.float32) / torch.tensor(tau_hidden_2, dtype=torch.float32))\n",
    "print(f\"\\nbeta_hidden_2: {beta_hidden_2}\")\n",
    "\n",
    "beta_hidden_3 = torch.exp(-torch.tensor(delaT, dtype=torch.float32) / torch.tensor(tau_hidden_3, dtype=torch.float32))\n",
    "print(f\"\\nbeta_hidden_3: {beta_hidden_3}\")\n",
    "\n",
    "tau_output = np.repeat(10, output_size)\n",
    "print(f\"\\ntau_output: {tau_output}\")\n",
    "\n",
    "beta_output = torch.exp(-torch.tensor(delaT, dtype=torch.float32) / torch.tensor(tau_output, dtype=torch.float32))\n",
    "print(f\"\\nbeta_output: {beta_output}\")\n",
    "\n",
    "cuba_beta = torch.exp(-torch.tensor(delaT, dtype=torch.float32) / torch.tensor(cuba_tau, dtype=torch.float32))\n",
    "print(f\"\\ncuba_beta: {cuba_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set random seeds for reproducibility\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# # Device configuration is handled by Lightning\n",
    "# # So we don't need to manually set device\n",
    "\n",
    "# # Initialize the data module\n",
    "# data_dir = 'data/4_one_second_samples' \n",
    "# data_module = QUTDataModule(\n",
    "#     data_dir, batch_size=batch_size, num_workers=num_workers\n",
    "# )\n",
    "\n",
    "# # Initialize the Lightning model\n",
    "# model = Lightning_SNNQUT(\n",
    "#     input_size=input_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     output_size=output_size,\n",
    "#     beta_hidden_1=beta_hidden_1,\n",
    "#     beta_hidden_2=beta_hidden_2,\n",
    "#     beta_hidden_3=beta_hidden_3,\n",
    "#     beta_output=beta_output,\n",
    "#     hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "#     output_reset_mechanism=output_reset_mechanism,\n",
    "#     learning_rate=learning_rate,\n",
    "#     optimizer_betas=optimizer_betas,\n",
    "#     scheduler_step_size=scheduler_step_size,\n",
    "#     scheduler_gamma=scheduler_gamma,\n",
    "#     output_threshold=output_threshold,\n",
    "#     cuba_beta=cuba_beta,\n",
    "#     hid_threshold = hid_threshold\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=num_epochs,\n",
    "#     # logger=logger,\n",
    "#     # Uncomment the following line if GPU is available\n",
    "#     # devices=1 if torch.cuda.is_available() else None,\n",
    "#     # accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "# )\n",
    "\n",
    "# # Start training\n",
    "# trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "# # Test the model\n",
    "# trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikel/Documents/GitHub/nni/nni/runtime/trial_command_channel/standalone.py:34: RuntimeWarning: \u001b[1m\u001b[31mRunning trial code without runtime. Please check the tutorial if you are new to NNI: \u001b[33mhttps://nni.readthedocs.io/en/stable/tutorials/hpo_quickstart_pytorch/main.html\u001b[0m\n",
      "  warnings.warn(warning_message, RuntimeWarning)\n",
      "Seed set to 42\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type    | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model         | SNNQUT  | 1.6 K  | train\n",
      "1 | loss_function | MSELoss | 0      | train\n",
      "--------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2400 files from data/TEST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c733f03fa542d5add1f635219b39bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NeuroVecio/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/opt/anaconda3/envs/NeuroVecio/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/opt/anaconda3/envs/NeuroVecio/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462b4a903f6d470c96ddf7c3965a94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87bc0e616934f4f863eb00d399231d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6491eb6e03c4b65955498a9d0828a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2400 files from data/TEST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73d807d94eb4b88982c2649bfaf09d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy           26.94444465637207\n",
      "     val_loss_epoch         43.750003814697266\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[2024-11-21 17:07:56] \u001b[32mFinal result: 26.94444465637207\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get hyperparameters from NNI\n",
    "    params = get_nni_params()\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Initialize the data module\n",
    "    #data_dir = 'data/4_one_second_samples'\n",
    "    data_dir = 'data/TEST'\n",
    "    data_module = QUTDataModule(\n",
    "        data_dir, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # Initialize the Lightning model with hyperparameters from NNI\n",
    "    model = Lightning_SNNQUT(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        beta_hidden_1=beta_hidden_1,\n",
    "        beta_hidden_2=beta_hidden_2,\n",
    "        beta_hidden_3=beta_hidden_3,\n",
    "        beta_output=beta_output,\n",
    "        hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "        output_reset_mechanism=output_reset_mechanism,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        optimizer_betas=tuple(params['optimizer_betas']),\n",
    "        scheduler_step_size=scheduler_step_size,\n",
    "        scheduler_gamma=scheduler_gamma,\n",
    "        output_threshold=output_threshold,\n",
    "        cuba_beta=cuba_beta,\n",
    "        hid_threshold=hid_threshold,\n",
    "        fast_sigmoid_slope=params['fast_sigmoid_slope'],\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        # devices and accelerator settings\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # Validate the model\n",
    "    #val_result = trainer.validate(model, datamodule=data_module)\n",
    "    # val_accuracy = val_result[0]['val_accuracy'] NOTE: Accessing val_accuracy as val_result[0]['val_accuracy'] only gives the accuracy for the first batch, not the entire validation dataset.\n",
    "\n",
    "    # Validate the model\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "    val_accuracy = trainer.callback_metrics['val_accuracy'].item() # Accessing val_accuracy as trainer.callback_metrics['val_accuracy'] gives the accuracy for the entire validation dataset. .item() is used to convert the tensor to a Python number.\n",
    "\n",
    "\n",
    "\n",
    "    # Report the result to NNI\n",
    "    nni.report_final_result(val_accuracy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XyloQUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
