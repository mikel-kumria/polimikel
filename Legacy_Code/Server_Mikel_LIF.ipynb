{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nni\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import ConfusionMatrix, Precision, Recall, F1Score\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from snntorch import surrogate\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "from Server_DataLoader import CustomDataModule, CLASSES\n",
    "from utils_metrics import compute_sharpness, compute_gradient_noise, visualize_loss_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Tensor Core optimizations for faster matrix multiplications.\n",
    "# This trades a tiny bit of precision (from fp32 to fp16) for significantly better performance on modern NVIDIA GPUs.\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters and Constants\n",
    "input_size = 16\n",
    "hidden_size = 24\n",
    "output_size = 4\n",
    "\n",
    "bit_width = 16 # n-bit quantization\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "scheduler_step_size = 20\n",
    "scheduler_gamma = 0.4\n",
    "\n",
    "output_threshold = 1e7\n",
    "hidden_threshold = 1\n",
    "\n",
    "#num_workers = max(1, os.cpu_count() - 1)\n",
    "num_workers = 1\n",
    "hidden_reset_mechanism = 'subtract'\n",
    "output_reset_mechanism = 'none'\n",
    "\n",
    "Vmem_shift_for_MSELoss = 0.2\n",
    "\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tau_beta_values(hidden_size, output_size):\n",
    "    def create_power_vector(n, size):\n",
    "        powers = [2 ** i for i in range(1, n + 1)]\n",
    "        repeat_count = size // n\n",
    "        power_vector = np.repeat(powers, repeat_count)\n",
    "        return power_vector\n",
    "\n",
    "    size = hidden_size\n",
    "    tau_hidden_1 = create_power_vector(n=2, size=size)\n",
    "    tau_hidden_2 = create_power_vector(n=4, size=size)\n",
    "    tau_hidden_3 = create_power_vector(n=8, size=size)\n",
    "\n",
    "    delta_t = 1\n",
    "    beta_hidden_1 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_1, dtype=torch.float32))\n",
    "    beta_hidden_2 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_2, dtype=torch.float32))\n",
    "    beta_hidden_3 = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_hidden_3, dtype=torch.float32))\n",
    "\n",
    "    tau_output = np.repeat(10, output_size)\n",
    "    beta_output = torch.exp(-torch.tensor(delta_t) / torch.tensor(tau_output, dtype=torch.float32))\n",
    "\n",
    "    return beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nni_params():\n",
    "    params = {\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer_betas': (0.9, 0.99),\n",
    "        'fast_sigmoid_slope': 10,\n",
    "    }\n",
    "    tuner_params = nni.get_next_parameter()\n",
    "    params.update(tuner_params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantization with fixed range.\n",
    "\n",
    "# class Fake_Quantize_n_bit(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.levels = 2 ** bit_width\n",
    "#         self.w_min = -0.5\n",
    "#         self.w_max = 1.0\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # Quantization steps:\n",
    "#         # Clamp to [w_min, w_max] to avoid going out of range.\n",
    "#         input_clamped = torch.clamp(input, self.w_min, self.w_max)\n",
    "#         scale = (self.w_max - self.w_min) / (self.levels - 1)\n",
    "#         quant_indices = torch.round((input_clamped - self.w_min) / scale)\n",
    "#         quant_w = quant_indices * scale + self.w_min\n",
    "#         return quant_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization with dynamic range, based on the min/max of the current layer.\n",
    "\n",
    "class Fake_Quantize_n_bit(nn.Module):\n",
    "    def __init__(self, bit_width=16):\n",
    "        super().__init__()\n",
    "        self.levels = 2 ** bit_width\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Compute dynamic min and max from the current layer's weights\n",
    "        w_min = input.min().item()\n",
    "        w_max = input.max().item()\n",
    "\n",
    "        # Compute scale based on dynamic range\n",
    "        scale = (w_max - w_min) / (self.levels - 1)\n",
    "        x = (input - w_min) / scale\n",
    "        # Map to quantization indices\n",
    "        quant_indices = x + torch.round(x).detach() - x.detach()\n",
    "        # Map back to the quantized range\n",
    "        quant_w = quant_indices * scale + w_min\n",
    "        return quant_w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "        self.fake_quant = Fake_Quantize_n_bit()\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant_weight = self.fake_quant(self.weight)\n",
    "        return nn.functional.linear(input, quant_weight, self.bias)\n",
    "\n",
    "def finalize_quantization(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'fake_quant') and hasattr(module, 'weight'):\n",
    "                quantized_weight = module.fake_quant(module.weight)\n",
    "                module.weight.data.copy_(quantized_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNQUT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output, \n",
    "                 hidden_reset_mechanism, output_reset_mechanism, \n",
    "                 hidden_threshold, output_threshold, fast_sigmoid_slope):\n",
    "        super().__init__()\n",
    "        self.fc1 = QuantLinear(input_size, hidden_size, bias=False)\n",
    "        #self.fc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta_hidden_1, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc2 = QuantLinear(hidden_size, hidden_size, bias=False)\n",
    "        #self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta_hidden_2, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc3 = QuantLinear(hidden_size, hidden_size, bias=False)\n",
    "        #self.fc3 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.lif3 = snn.Leaky(beta=beta_hidden_3, reset_mechanism=hidden_reset_mechanism,\n",
    "                              threshold=hidden_threshold, \n",
    "                              spike_grad=surrogate.fast_sigmoid(slope=fast_sigmoid_slope))\n",
    "\n",
    "        self.fc4 = QuantLinear(hidden_size, output_size, bias=False)\n",
    "        #self.fc4 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.lif4 = snn.Leaky(beta=beta_output, reset_mechanism=output_reset_mechanism,\n",
    "                              threshold=output_threshold)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        mem1 = torch.zeros(batch_size, self.fc1.out_features, device=device)\n",
    "        mem2 = torch.zeros(batch_size, self.fc2.out_features, device=device)\n",
    "        mem3 = torch.zeros(batch_size, self.fc3.out_features, device=device)\n",
    "        mem4 = torch.zeros(batch_size, self.fc4.out_features, device=device)\n",
    "\n",
    "        spk1_rec, spk2_rec, spk3_rec, spk4_rec, mem4_rec = [], [], [], [], []\n",
    "\n",
    "        for step in range(time_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            spk4_rec.append(spk4)\n",
    "            mem4_rec.append(mem4)\n",
    "\n",
    "        return (torch.stack(spk1_rec, dim=0),\n",
    "                torch.stack(spk2_rec, dim=0),\n",
    "                torch.stack(spk3_rec, dim=0),\n",
    "                torch.stack(spk4_rec, dim=0),\n",
    "                torch.stack(mem4_rec, dim=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lightning_SNNQUT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        beta_hidden_1,\n",
    "        beta_hidden_2,\n",
    "        beta_hidden_3,\n",
    "        beta_output,\n",
    "        hidden_reset_mechanism,\n",
    "        output_reset_mechanism,\n",
    "        hidden_threshold,\n",
    "        output_threshold,\n",
    "        learning_rate,\n",
    "        scheduler_step_size,\n",
    "        scheduler_gamma,\n",
    "        optimizer_betas,\n",
    "        fast_sigmoid_slope,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SNNQUT(\n",
    "            input_size=self.hparams.input_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            beta_hidden_1=beta_hidden_1,\n",
    "            beta_hidden_2=beta_hidden_2,\n",
    "            beta_hidden_3=beta_hidden_3,\n",
    "            beta_output=beta_output,\n",
    "            hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "            output_reset_mechanism=output_reset_mechanism,\n",
    "            output_threshold=output_threshold,\n",
    "            hidden_threshold=hidden_threshold,\n",
    "            fast_sigmoid_slope=fast_sigmoid_slope,\n",
    "        )\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.train_confmat = ConfusionMatrix(task='multiclass', num_classes=self.hparams.output_size)\n",
    "        self.val_confmat = ConfusionMatrix(task='multiclass', num_classes=self.hparams.output_size)\n",
    "        self.test_confmat = ConfusionMatrix(task='multiclass', num_classes=self.hparams.output_size)\n",
    "\n",
    "        # Precision, recall, F1\n",
    "        self.precision_metric = Precision(task='multiclass', num_classes=self.hparams.output_size, average=None)\n",
    "        self.recall_metric = Recall(task='multiclass', num_classes=self.hparams.output_size, average=None)\n",
    "        self.f1_metric = F1Score(task='multiclass', num_classes=self.hparams.output_size, average=None)\n",
    "\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss_and_metrics(self, spk4_rec, mem4_rec, labels, mode='Vmem',dict_spkcount=None):\n",
    "        labels_expanded = labels.unsqueeze(0).expand(mem4_rec.size(0), -1, -1)\n",
    "        loss = self.loss_function(mem4_rec, (labels_expanded + Vmem_shift_for_MSELoss))\n",
    "        if dict_spkcount != None:\n",
    "            reg = (dict_spkcount['spk1'] + dict_spkcount['spk2'] + dict_spkcount['spk3'] + dict_spkcount['spk4'])**2*1e-12\n",
    "            #print(\"loss:\", loss)\n",
    "            #print(\"reg\",reg)\n",
    "            loss += reg\n",
    "        if mode == 'Vmem':\n",
    "            final_out = mem4_rec.sum(0)\n",
    "        elif mode == 'spike':\n",
    "            final_out = spk4_rec.sum(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}, use 'Vmem' or 'spike'\")\n",
    "\n",
    "        _, predicted = final_out.max(-1)\n",
    "        _, targets = labels.max(-1)\n",
    "\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.numel()\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "        return loss, accuracy, predicted, targets\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1, spk2, spk3, spk4, mem4 = self(inputs)\n",
    "        dict_spkcount = {'spk1': spk1.sum().item(), 'spk2': spk2.sum().item(), 'spk3': spk3.sum().item(), 'spk4': spk4.sum().item()}\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4, mem4, labels, mode='Vmem',dict_spkcount=dict_spkcount)\n",
    "        self.train_confmat.update(predicted, targets)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_accuracy', accuracy*100, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Weight distributions logging\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                self.logger.experiment.add_histogram(f'weights/{name}', param, self.current_epoch)\n",
    "\n",
    "        # Gradient histograms logging\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                self.logger.experiment.add_histogram(f'grads/{name}', param.grad, self.current_epoch)\n",
    "\n",
    "        # Runtime per epoch logging\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.log('epoch_time', epoch_time, on_epoch=True)\n",
    "\n",
    "        # Memory usage logging\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated()/(1024**2)\n",
    "            self.log('gpu_memory_MB', mem_alloc, on_epoch=True)\n",
    "        process = psutil.Process(os.getpid())\n",
    "        cpu_mem = process.memory_info().rss / (1024**2)\n",
    "        self.log('cpu_memory_MB', cpu_mem, on_epoch=True)\n",
    "\n",
    "        # Learning rate logging\n",
    "        optimizer = self.optimizers()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        self.log('learning_rate', lr, on_epoch=True)\n",
    "\n",
    "        self.train_confmat.reset()\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_confmat.reset()\n",
    "\n",
    "        # Logging membrane potentials and spiking activity for a small batch\n",
    "        sample_batch = next(iter(self.trainer.datamodule.val_dataloader()))\n",
    "        sample_input, sample_labels = sample_batch\n",
    "        sample_input = sample_input.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            spk1, spk2, spk3, spk4, mem4 = self(sample_input)\n",
    "\n",
    "\n",
    "            self.logger.experiment.add_histogram('mem4_distribution', mem4.cpu(), self.current_epoch)\n",
    "            self.logger.experiment.add_histogram('spk4_distribution', spk4.cpu().float(), self.current_epoch)\n",
    "            self.logger.experiment.add_scalar('spk1_count', spk1.sum().cpu(), self.current_epoch)\n",
    "            self.logger.experiment.add_scalar('spk2_count', spk2.sum().cpu(), self.current_epoch)\n",
    "            self.logger.experiment.add_scalar('spk3_count', spk3.sum().cpu(), self.current_epoch)\n",
    "            self.logger.experiment.add_scalar('spk4_count', spk4.sum().cpu(), self.current_epoch)\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        # Re-quantize the model weights after backward to ensure no drift from quantized levels\n",
    "        finalize_quantization(self.model)\n",
    "\n",
    "        # Logging effective step size = lr * grad_norm\n",
    "        optimizer = self.optimizers()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        total_grad_norm = 0.0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item()**2\n",
    "        total_grad_norm = total_grad_norm**0.5\n",
    "        effective_step_size = lr * total_grad_norm\n",
    "        self.log('effective_step_size', effective_step_size, on_epoch=True)\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        # Final quantization of weights and model saving\n",
    "        finalize_quantization(self.model)\n",
    "        model_path = os.path.join(self.logger.save_dir, self.logger.name, f\"final_quantized_model_epoch{self.current_epoch}.pt\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        # # Compute sharpness and gradient noise on a small batch\n",
    "        # train_loader = self.trainer.datamodule.train_dataloader()\n",
    "        # inputs, targets = next(iter(train_loader))\n",
    "        # inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "        # sharpness = compute_sharpness(self.model, inputs, targets, self.loss_function, epsilon=1e-3)\n",
    "        # self.logger.experiment.add_scalar('sharpness', sharpness, self.current_epoch)\n",
    "\n",
    "\n",
    "        # gradient_noise = compute_gradient_noise(self.model, train_loader, self.loss_function, device=self.device, num_batches=5)\n",
    "        # self.logger.experiment.add_scalar('gradient_noise', gradient_noise, self.current_epoch)\n",
    "\n",
    "        # # Visualize loss landscape\n",
    "        # fig = visualize_loss_landscape(self.model, inputs, targets, self.loss_function, d1_scale=0.05, d2_scale=0.05, steps=5)\n",
    "        # self.logger.experiment.add_figure('loss_landscape', fig, self.current_epoch)\n",
    "        # plt.close(fig)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1, spk2, spk3, spk4, mem4 = self(inputs)\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4, mem4, labels, mode='Vmem')\n",
    "        self.val_confmat.update(predicted, targets)\n",
    "        self.precision_metric.update(predicted, targets)\n",
    "        self.recall_metric.update(predicted, targets)\n",
    "        self.f1_metric.update(predicted, targets)\n",
    "\n",
    "        final_out = mem4.sum(0)\n",
    "\n",
    "        if batch_idx == 0:  # once per epoch print some info to see what's happening\n",
    "            print(\"final_out:\", final_out[:5])    # print only first 5 samples\n",
    "            print(\"targets:\", labels[:5])\n",
    "            print(\"predicted classes:\", predicted[:10])\n",
    "            print(\"true classes:\", targets[:10])\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy * 100, on_epoch=True, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        spk1, spk2, spk3, spk4, mem4 = self(inputs)\n",
    "        loss, accuracy, predicted, targets = self.compute_loss_and_metrics(spk4, mem4, labels, mode='spike')\n",
    "        self.test_confmat.update(predicted, targets)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy * 100, on_epoch=True, prog_bar=True)\n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adamax(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=self.hparams.optimizer_betas,\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hparams.scheduler_step_size,\n",
    "            gamma=self.hparams.scheduler_gamma,\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    params = get_nni_params()\n",
    "    beta_hidden_1, beta_hidden_2, beta_hidden_3, beta_output = generate_tau_beta_values(hidden_size, output_size)\n",
    "\n",
    "    train_val_dir = 'data/YES_COCHLEA_DATASET/TRAIN_VAL'\n",
    "    test_dir = 'data/YES_COCHLEA_DATASET/TEST'\n",
    "\n",
    "    data_module = CustomDataModule(\n",
    "        train_val_dir=train_val_dir,\n",
    "        test_dir=test_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        seed=SEED\n",
    "    )\n",
    "    data_module.setup('fit')\n",
    "\n",
    "    model = Lightning_SNNQUT(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        beta_hidden_1=beta_hidden_1,\n",
    "        beta_hidden_2=beta_hidden_2,\n",
    "        beta_hidden_3=beta_hidden_3,\n",
    "        beta_output=beta_output,\n",
    "        hidden_reset_mechanism=hidden_reset_mechanism,\n",
    "        output_reset_mechanism=output_reset_mechanism,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        optimizer_betas=params['optimizer_betas'],\n",
    "        scheduler_step_size=scheduler_step_size,\n",
    "        scheduler_gamma=scheduler_gamma,\n",
    "        output_threshold=output_threshold,\n",
    "        hidden_threshold=hidden_threshold,\n",
    "        fast_sigmoid_slope=params['fast_sigmoid_slope'],\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir='logs', name='Mikel_LIF_quant_5bit')\n",
    "    trainer = pl.Trainer(max_epochs=num_epochs, log_every_n_steps=10, logger=logger)\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # At this point, the model is final_quantized and saved in on_fit_end. We can now run the separate test script after restarting.\n",
    "\n",
    "    # Perform validation before finishing:\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "    val_accuracy = trainer.callback_metrics.get('val_accuracy', torch.tensor(0)).item()\n",
    "\n",
    "    nni.report_final_result(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                      | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model            | SNNQUT                    | 1.6 K  | train\n",
      "1 | loss_function    | MSELoss                   | 0      | train\n",
      "2 | train_confmat    | MulticlassConfusionMatrix | 0      | train\n",
      "3 | val_confmat      | MulticlassConfusionMatrix | 0      | train\n",
      "4 | test_confmat     | MulticlassConfusionMatrix | 0      | train\n",
      "5 | precision_metric | MulticlassPrecision       | 0      | train\n",
      "6 | recall_metric    | MulticlassRecall          | 0      | train\n",
      "7 | f1_metric        | MulticlassF1Score         | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]final_out: tensor([[-1503.1726,  1637.3700,  1434.5337,  5215.9688],\n",
      "        [ -936.0920,  1092.3643,  1480.9445,  4592.2100],\n",
      "        [-2756.4077,  3505.3445,  1363.5493,  7212.2573],\n",
      "        [-1391.9362,  1573.7344,  1605.1852,  5164.8145],\n",
      "        [-1574.7911,  1828.5530,  1567.7847,  5513.4351]], device='cuda:0')\n",
      "targets: tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "predicted classes: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "true classes: tensor([1, 3, 0, 1, 0, 3, 1, 2, 0, 1], device='cuda:0')\n",
      "Epoch 0:   0%|          | 0/576 [00:00<?, ?it/s]                           loss: tensor(15.6338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.455279465536\n",
      "Epoch 0:   0%|          | 1/576 [00:01<09:50,  0.97it/s, v_num=26, train_accuracy_step=18.80]loss: tensor(11.1601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.464945333161\n",
      "Epoch 0:   0%|          | 2/576 [00:02<09:39,  0.99it/s, v_num=26, train_accuracy_step=37.50]loss: tensor(10.3740, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.459202746025\n",
      "Epoch 0:   1%|          | 3/576 [00:02<09:30,  1.00it/s, v_num=26, train_accuracy_step=15.60]loss: tensor(8.0951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.432846883921\n",
      "Epoch 0:   1%|          | 4/576 [00:03<09:29,  1.00it/s, v_num=26, train_accuracy_step=25.00]loss: tensor(6.0955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.427421750625\n",
      "Epoch 0:   1%|          | 5/576 [00:04<09:29,  1.00it/s, v_num=26, train_accuracy_step=25.00]loss: tensor(5.5796, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.429610769809\n",
      "Epoch 0:   1%|          | 6/576 [00:06<09:47,  0.97it/s, v_num=26, train_accuracy_step=34.40]loss: tensor(4.8899, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.4373573689\n",
      "Epoch 0:   1%|          | 7/576 [00:07<09:41,  0.98it/s, v_num=26, train_accuracy_step=34.40]loss: tensor(4.6064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.425406581824\n",
      "Epoch 0:   1%|▏         | 8/576 [00:08<09:35,  0.99it/s, v_num=26, train_accuracy_step=25.00]loss: tensor(4.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.42899094067599997\n",
      "Epoch 0:   2%|▏         | 9/576 [00:09<09:31,  0.99it/s, v_num=26, train_accuracy_step=15.60]loss: tensor(3.0477, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.429000110361\n",
      "Epoch 0:   2%|▏         | 10/576 [00:10<09:27,  1.00it/s, v_num=26, train_accuracy_step=37.50]loss: tensor(2.8596, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.4043942464\n",
      "Epoch 0:   2%|▏         | 11/576 [00:10<09:24,  1.00it/s, v_num=26, train_accuracy_step=40.60]loss: tensor(2.9495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.40195092801599996\n",
      "Epoch 0:   2%|▏         | 12/576 [00:11<09:21,  1.01it/s, v_num=26, train_accuracy_step=21.90]loss: tensor(2.5539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.413006734336\n",
      "Epoch 0:   2%|▏         | 13/576 [00:12<09:18,  1.01it/s, v_num=26, train_accuracy_step=31.20]loss: tensor(2.1567, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.42310081344399997\n",
      "Epoch 0:   2%|▏         | 14/576 [00:14<09:22,  1.00it/s, v_num=26, train_accuracy_step=31.20]loss: tensor(2.5947, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.407028688144\n",
      "Epoch 0:   3%|▎         | 15/576 [00:14<09:19,  1.00it/s, v_num=26, train_accuracy_step=9.380]loss: tensor(2.1911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.404503632036\n",
      "Epoch 0:   3%|▎         | 16/576 [00:15<09:17,  1.00it/s, v_num=26, train_accuracy_step=9.380]loss: tensor(2.0259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.435753133456\n",
      "Epoch 0:   3%|▎         | 17/576 [00:16<09:14,  1.01it/s, v_num=26, train_accuracy_step=6.250]loss: tensor(2.0330, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.420222876516\n",
      "Epoch 0:   3%|▎         | 18/576 [00:17<09:12,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(1.8362, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.401011895025\n",
      "Epoch 0:   3%|▎         | 19/576 [00:18<09:11,  1.01it/s, v_num=26, train_accuracy_step=6.250]loss: tensor(1.6608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.414229970449\n",
      "Epoch 0:   3%|▎         | 20/576 [00:19<09:09,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(1.6772, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.408396405481\n",
      "Epoch 0:   4%|▎         | 21/576 [00:20<09:07,  1.01it/s, v_num=26, train_accuracy_step=9.380]loss: tensor(1.4659, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.40187358422499997\n",
      "Epoch 0:   4%|▍         | 22/576 [00:21<09:09,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(1.6826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.384994189441\n",
      "Epoch 0:   4%|▍         | 23/576 [00:22<09:08,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(1.3310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.396415048225\n",
      "Epoch 0:   4%|▍         | 24/576 [00:23<09:07,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(1.6348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.372616238929\n",
      "Epoch 0:   4%|▍         | 25/576 [00:24<09:06,  1.01it/s, v_num=26, train_accuracy_step=9.380]loss: tensor(1.3116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.400591523929\n",
      "Epoch 0:   5%|▍         | 26/576 [00:25<09:04,  1.01it/s, v_num=26, train_accuracy_step=6.250]loss: tensor(1.2661, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.383678658724\n",
      "Epoch 0:   5%|▍         | 27/576 [00:26<09:03,  1.01it/s, v_num=26, train_accuracy_step=6.250]loss: tensor(1.4105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.368234153329\n",
      "Epoch 0:   5%|▍         | 28/576 [00:27<09:02,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(1.2625, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.377956907524\n",
      "Epoch 0:   5%|▌         | 29/576 [00:28<09:00,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(1.1583, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.3862498201\n",
      "Epoch 0:   5%|▌         | 30/576 [00:29<09:02,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(1.0512, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.386303270089\n",
      "Epoch 0:   5%|▌         | 31/576 [00:30<09:01,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(1.3407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.364013122225\n",
      "Epoch 0:   6%|▌         | 32/576 [00:31<09:00,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(1.0818, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.386406451456\n",
      "Epoch 0:   6%|▌         | 33/576 [00:32<08:59,  1.01it/s, v_num=26, train_accuracy_step=12.50]loss: tensor(0.9220, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.374206252176\n",
      "Epoch 0:   6%|▌         | 34/576 [00:33<08:57,  1.01it/s, v_num=26, train_accuracy_step=3.120]loss: tensor(0.8802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.37862977824099997\n",
      "Epoch 0:   6%|▌         | 35/576 [00:34<08:56,  1.01it/s, v_num=26, train_accuracy_step=18.80]loss: tensor(0.8677, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "reg 0.383788923049\n",
      "Epoch 0:   6%|▋         | 36/576 [00:35<08:55,  1.01it/s, v_num=26, train_accuracy_step=9.380]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Step 1: Generate a random normal (Gaussian) distribution of values\n",
    "# torch.manual_seed(0)  # For reproducibility\n",
    "# original_values = torch.randn(10000) * 0.2 + 0.5  # mean=0.5, std=0.2\n",
    "\n",
    "# # Step 2: Define a 5-bit quantization function\n",
    "# def quantize_5bit(x, w_min=0.001, w_max=1.0, levels=16):\n",
    "#     # Ensure x is within the defined range\n",
    "#     x_clamped = torch.clamp(x, w_min, w_max)\n",
    "#     scale = (w_max - w_min) / (levels - 1)\n",
    "#     # Map to quantization indices\n",
    "#     quant_indices = torch.round((x_clamped - w_min) / scale)\n",
    "#     # Map back to original range\n",
    "#     quant_x = quant_indices * scale + w_min\n",
    "#     return quant_x\n",
    "\n",
    "# # Step 3: Apply quantization\n",
    "# quantized_values = quantize_5bit(original_values)\n",
    "\n",
    "# # Step 4: Plot histograms\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# axes[0].hist(original_values.numpy(), bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "# axes[0].set_title('Original Distribution')\n",
    "# axes[0].set_xlabel('Value')\n",
    "# axes[0].set_ylabel('Count')\n",
    "\n",
    "# axes[1].hist(quantized_values.numpy(), bins=32, color='green', alpha=0.7, edgecolor='black')\n",
    "# axes[1].set_title('Quantized (5-bit) Distribution')\n",
    "# axes[1].set_xlabel('Value')\n",
    "# axes[1].set_ylabel('Count')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Define your classes here\n",
    "# CLASSES = [\"CAR\", \"STREET\", \"HOME\", \"CAFE\"]\n",
    "\n",
    "# def get_label_from_folder(folder_name):\n",
    "#     # Extract label from folder name by splitting at '-'\n",
    "#     # Adjust this logic as needed for your naming convention.\n",
    "#     base = os.path.basename(folder_name)\n",
    "#     label = base.split('-')[0]\n",
    "#     return label\n",
    "\n",
    "# def one_hot_encode(label):\n",
    "#     idx = CLASSES.index(label)\n",
    "#     vec = np.zeros(len(CLASSES), dtype=np.float32)\n",
    "#     vec[idx] = 1.0\n",
    "#     return vec\n",
    "\n",
    "# class CustomSNNTrainValDataset(Dataset):\n",
    "#     def __init__(self, root_dir, class_list=CLASSES, time_steps=1000, input_dim=16, debug=False):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.class_list = class_list\n",
    "#         self.time_steps = time_steps\n",
    "#         self.input_dim = input_dim\n",
    "#         self.debug = debug\n",
    "#         self.samples = []\n",
    "\n",
    "#         # Collect all CSV files and their labels\n",
    "#         for folder in os.listdir(self.root_dir):\n",
    "#             folder_path = os.path.join(self.root_dir, folder)\n",
    "#             if not os.path.isdir(folder_path):\n",
    "#                 continue\n",
    "#             label = get_label_from_folder(folder_path)\n",
    "#             if label not in self.class_list:\n",
    "#                 continue\n",
    "#             label_vec = one_hot_encode(label)\n",
    "#             csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "#             for csv_file in csv_files:\n",
    "#                 data_array = self.load_csv(csv_file, self.time_steps, self.input_dim)\n",
    "#                 self.samples.append((csv_file, data_array, label_vec))\n",
    "\n",
    "#     def load_csv(self, csv_path, time_steps, input_dim):\n",
    "#         data = np.loadtxt(csv_path, delimiter=',', dtype=np.float32)\n",
    "#         if data.shape != (time_steps, input_dim):\n",
    "#             raise ValueError(f\"CSV {csv_path} shape mismatch: {data.shape}\")\n",
    "#         return data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         csv_file, data, label = self.samples[idx]\n",
    "#         if self.debug:\n",
    "#             print(f\"[DEBUG] Loading file: {csv_file}\")\n",
    "#             print(f\"[DEBUG] Data shape: {data.shape}\")\n",
    "#             print(f\"[DEBUG] Label: {label}\")\n",
    "#         return torch.tensor(data), torch.tensor(label)\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Replace with your actual directory\n",
    "#     train_val_dir = 'data/YES_COCHLEA_DATASET/TRAIN_VAL'\n",
    "    \n",
    "#     # Create the dataset with debug mode enabled\n",
    "#     dataset = CustomSNNTrainValDataset(train_val_dir, debug=True)\n",
    "\n",
    "#     # Wrap in DataLoader\n",
    "#     dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "#     # Inspect some batches\n",
    "#     # This will trigger prints from the dataset __getitem__ if debug=True\n",
    "#     for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "#         print(f\"Batch {batch_idx}:\")\n",
    "#         print(f\"  inputs shape: {inputs.shape} (expected [batch_size, time_steps, input_dim])\")\n",
    "#         print(f\"  labels shape: {labels.shape} (expected [batch_size, num_classes])\")\n",
    "\n",
    "#         # Print sample values\n",
    "#         # Inputs is typically [batch, time_steps, input_dim]\n",
    "#         # Labels is [batch, num_classes]\n",
    "#         print(f\"  Sample input[0, 0, :5]: {inputs[0, 0, :5]}\")\n",
    "#         print(f\"  Sample label[0]: {labels[0]}\")\n",
    "\n",
    "#         # After a couple of batches, stop to avoid too much output\n",
    "#         if batch_idx == 2:\n",
    "#             break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
